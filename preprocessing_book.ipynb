{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfa19caa-6437-4f16-b6d5-01347159d330",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e447f2f-9f8e-4398-aaf6-21b33ef3ea41",
   "metadata": {},
   "source": [
    "The preprocessing pipeline should output a list of speakers with quotes. Each element of this list should contain information about the speaker (full name, gender, date of birth...) - obtained from wikidata, and a string formed by joining multiple quotes in order to get 5000 characters of text (restriction of the LIWC model)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80871718-bd1a-4806-b4be-c75f898ecc9a",
   "metadata": {},
   "source": [
    "The pipeline is presented through a data analysis example - analysing the personalities of the US politicians. In this example we take 100 politicians from both of the two major political parties, the Democratic party and the Republican party. We select the politicians which have the most quotes in our database. We only consider quotes for which the probability of the speaker is higher than 80% (referenced as significant quotes in this notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1bab713-bee5-46eb-a26f-446db88ac641",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bz2\n",
    "import json\n",
    "import re\n",
    "import random\n",
    "import sys\n",
    "import os\n",
    "import bz2\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d57b92-b15c-4f8e-8195-cfd3a7669877",
   "metadata": {},
   "source": [
    "### Counting significant quotes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08fdb05-f8e7-42fb-9f2b-2f00a60bde59",
   "metadata": {},
   "source": [
    "<b>This step has been executed once and will not be needed in following analyses since the quote counts calculated are for all the speakers and the output can be simply reused.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f263f01-5c14-48e6-8cfb-c6b813e17cb4",
   "metadata": {},
   "source": [
    "Define some methods for better reusability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4d02c4c6-18f0-4fbe-b387-a6fa98151145",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATTERN_INPUT = \"../quotebank/quotes-{}.json.bz2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90d36977-e339-4f1e-bfcd-85c0e38b86e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_json_to_file(name, obj):\n",
    "    # Use current timestamp to make the name of the file unique\n",
    "    millis = round(time.time() * 1000)\n",
    "    name = f'{name}_{millis}.json'\n",
    "    with open(name, 'wb') as f:\n",
    "        output = json.dumps(obj)\n",
    "        f.write(output.encode('utf-8'))\n",
    "    return name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51a096f-ec25-4b33-b006-dc2afc478a0a",
   "metadata": {},
   "source": [
    "Methods used for counting significant quotes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48807770-27e6-4cf5-b4b8-c6508f21db8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "signi_count = 0\n",
    "signi_quote_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0a00f07c-1f31-43fd-bb0a-701b9159b41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The signature remains from an older version of the code, parameter out_file could be removed, but then has to be removed in other places in the code as well.\n",
    "def initialize(out_file):\n",
    "    global signi_count\n",
    "    global signi_quote_dict\n",
    "    signi_count = 0\n",
    "    signi_quote_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c1b73b84-387f-4bf1-b7e3-46901c39ca9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The signature remains from an older version of the code, parameter out_file could be removed, but then has to be removed in other places in the code as well.\n",
    "def count_significant_quotes(out_file, row):\n",
    "    global signi_count\n",
    "    global signi_quote_dict\n",
    "    \n",
    "    probas = row['probas']\n",
    "    qids = row['qids']\n",
    "    \n",
    "    if (len(probas) == 0 or len(qids) == 0):\n",
    "        return\n",
    "    \n",
    "    if (probas[0][0] == 'None'):\n",
    "        return\n",
    "    \n",
    "    p = float(probas[0][1])\n",
    "    if (p < 0.8):\n",
    "        return\n",
    "    \n",
    "    qid = qids[0]\n",
    "    \n",
    "    signi_count = signi_count + 1\n",
    "    signi_quote_dict[qid] = signi_quote_dict.get(qid, 0) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d915fd-fc0e-4ed3-b3da-590afc1e309c",
   "metadata": {},
   "source": [
    "General methods used for processing the quotes files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5b902ef-f2a5-4eb0-b885-3c7d83326311",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Process a chunk of the input stream.\n",
    "\"\"\"\n",
    "def proc(input, evaluate_quote, max_length=20):\n",
    "    # Ugly global variable usage :(\n",
    "    global index\n",
    "    global invalid_json_count\n",
    "    global invalid_chunk_count\n",
    "    global chunk_stitching\n",
    "    global stitch_length\n",
    "    global scrap_next\n",
    "    global quote_is_open\n",
    "    global quote_part\n",
    "    global dat_part\n",
    "    global euro_error\n",
    "    global euro_count\n",
    "    \n",
    "    global totin\n",
    "    global totout\n",
    "    global prev\n",
    "    global dec\n",
    "    global start\n",
    "    \"\"\"Decompress and process a piece of a compressed stream\"\"\"\n",
    "    dat = dec.decompress(input)\n",
    "    got = len(dat)\n",
    "    if got != 0:    # 0 is common -- waiting for a bzip2 block\n",
    "        try:\n",
    "            if (euro_error):\n",
    "                # If the previous chunk ended unexpectedly and could not be decoded, try to combine it with this chunk\n",
    "                s = (dat_part + dat).decode('utf-8')\n",
    "                euro_error = False\n",
    "            else:\n",
    "                # Decode the current chunk\n",
    "                s = dat.decode('utf-8')\n",
    "                \n",
    "            # List elements in the quote files are separated by new lines (\\n)\n",
    "            lines = s.split('\\n')\n",
    "\n",
    "            for line in lines:\n",
    "                try:\n",
    "                    if (scrap_next):\n",
    "                        # If the object spans too many chunks we decide to scrap it, and keep scraping until JSON can parse the line (chunk)\n",
    "                        ob = json.loads(line)\n",
    "                        scrap_next = False\n",
    "                        quote_is_open = False\n",
    "                        chunk_stitching -= stitch_length\n",
    "                    else:\n",
    "                        if (quote_is_open):\n",
    "                            # If previous chunk ended in the middle of a JSON object we merge that content with the current line\n",
    "                            ob = json.loads(quote_part + line)\n",
    "                            quote_is_open = False\n",
    "                        else:\n",
    "                            # Parse the current line\n",
    "                            ob = json.loads(line)\n",
    "\n",
    "                    # Parametrization - do work on a single quote JSON object\n",
    "                    evaluate_quote({}, ob)\n",
    "                except ValueError:\n",
    "                    \"\"\"\n",
    "                    Error occurs when the line does not contain the whole JSON object, which happens for the last line in almost every chunk of input stream.\n",
    "                    We solve this by remembering the partial object, and then merging it with the rest of the object when we load the next chunk.\n",
    "                    JSON object might span more than 2 chunks, and in that case we keep merging until we reach max_length chunks, when we just throw away the object\n",
    "                    and count it as invalid using invalid_json_count.\n",
    "                    \"\"\"\n",
    "                    if (scrap_next):\n",
    "                        pass\n",
    "                    else:\n",
    "                        if (quote_is_open):\n",
    "                            chunk_stitching += 1\n",
    "                            quote_part = quote_part + line\n",
    "                            stitch_length += 1\n",
    "\n",
    "                            if (stitch_length > max_length):\n",
    "                                invalid_json_count += 1\n",
    "                                scrap_next = True\n",
    "                        else:\n",
    "                            quote_is_open = True\n",
    "                            quote_part = line\n",
    "                            stitch_length = 0\n",
    "        except UnicodeDecodeError as e:\n",
    "            # Error occurs when input stream is split in the middle of a character which is encoded with multiple bytes, for example the euro symbol\n",
    "            if (euro_error):\n",
    "                dat_part = dat_part + dat\n",
    "            else:\n",
    "                euro_error = True\n",
    "                dat_part = dat\n",
    "            \n",
    "            euro_count += 1\n",
    "        \n",
    "        index += 1\n",
    "    return got"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cc51b57e-891d-419f-8f33-88b098afba19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_through_quotes(init, evaluate_quote, year, target_dict_name, path_to_input, name='test', chunk_size=16384):\n",
    "    global index\n",
    "    global invalid_json_count\n",
    "    global invalid_chunk_count\n",
    "    global chunk_stitching\n",
    "    global stitch_length\n",
    "    global scrap_next\n",
    "    global quote_is_open\n",
    "    global quote_part\n",
    "    global dat_part\n",
    "    global euro_error\n",
    "    global euro_count\n",
    "    \n",
    "    global totin\n",
    "    global totout\n",
    "    global prev\n",
    "    global dec\n",
    "    global start\n",
    "    \n",
    "    size = os.path.getsize(path_to_input)\n",
    "    invalid_json_count = 0\n",
    "    invalid_chunk_count = 0\n",
    "    chunk_stitching = 0\n",
    "    stitch_length = 0\n",
    "    scrap_next = False\n",
    "    quote_is_open = False\n",
    "    quote_part = ''\n",
    "    dat_part = 0\n",
    "    euro_error = False\n",
    "    euro_count = 0\n",
    "    \n",
    "    totin = 0\n",
    "    totout = 0\n",
    "    prev = -1\n",
    "    dec = bz2.BZ2Decompressor()\n",
    "    start = time.time()\n",
    "    \n",
    "    init({})\n",
    "    \n",
    "    target_dict = poli_quotes if target_dict_name == \"poli_quotes\" else signi_quote_dict\n",
    "    index = 0\n",
    "    with open(path_to_input, 'rb') as f:\n",
    "        for chunk in iter(lambda: f.read(chunk_size), b''):\n",
    "            # feed chunk to decompressor\n",
    "            got = proc(chunk, evaluate_quote)\n",
    "\n",
    "            # handle case of concatenated bz2 streams\n",
    "            if dec.eof:\n",
    "                rem = dec.unused_data\n",
    "                dec = bz2.BZ2Decompressor()\n",
    "                got += proc(rem, evaluate_quote)\n",
    "\n",
    "            # show progress\n",
    "            totin += len(chunk)\n",
    "            totout += got\n",
    "            if got != 0:    # only if a bzip2 block emitted\n",
    "                frac = round(1000 * totin / size)\n",
    "                if frac != prev:\n",
    "                    left = (size / totin - 1) * (time.time() - start)\n",
    "                    print(f'\\r{frac / 10:.1f}% (~{left:.1f}s left)\\tyear: {year}\\tnumber of speakers: {len(target_dict)}\\tstitching: {chunk_stitching}\\teuro count: {euro_count}\\tinvalid json count: {invalid_json_count}\\tinvalid chunk count: {invalid_chunk_count}', end='')\n",
    "                    prev = frac\n",
    "\n",
    "    # Show the resulting size.\n",
    "    print(end='\\r')\n",
    "    print(totout, 'uncompressed bytes')\n",
    "\n",
    "    output_name = write_json_to_file(f'{name}-{year}', target_dict)\n",
    "    return output_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7328f060-7f5b-49d3-94b8-a9633427bd5e",
   "metadata": {},
   "source": [
    "Create files for every year, each file contains a dictionary where the key is the QID of the speaker, and the value is the number of significant quotes.\n",
    "<br><br>\n",
    "<font color='red'>WARNING: LONG EXECUTION!</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d8f659-4c51-4fb1-9baa-5e7bbf86fca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "years = [2015, 2016, 2017, 2018, 2019, 2020]\n",
    "# years = [2020]\n",
    "for year in years:\n",
    "    path_to_input = PATTERN_INPUT.format(year)\n",
    "    \n",
    "    run_through_quotes(\n",
    "        initialize, count_significant_quotes, year, \"signi_quote_dict\", path_to_input, name='signi-quote-count', chunk_size=1_048_576)\n",
    "    print('')\n",
    "    print(f'Finished compiling quotes for the year {year}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11b13ee-41a4-4d9e-969b-ee949ebe30f9",
   "metadata": {},
   "source": [
    "Now combine the quote counts into a single file.\n",
    "<br>\n",
    "An example of the file names is used, the string should be updated if the code is ran again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "88727e52-19fb-447a-b0a4-653582d13d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "signi_quotes_file_names = [\n",
    "    \"signi-quote-count-2015_1636244638891.json\",\n",
    "    \"signi-quote-count-2016_1636246832187.json\",\n",
    "    \"signi-quote-count-2017_1636249273913.json\",\n",
    "    \"signi-quote-count-2018_1636250518608.json\",\n",
    "    \"signi-quote-count-2019_1636251729971.json\",\n",
    "    \"signi-quote-count-2020_1636237785105.json\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b0d00c-0663-476f-ad5d-f6afd69dee24",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_signi_dict = {}\n",
    "\n",
    "for file_name in signi_quotes_file_names:\n",
    "    with open(file_name, 'r') as f:\n",
    "        one_dict = json.load(f)\n",
    "        for k in one_dict.keys():\n",
    "            combined_signi_dict[k] = combined_signi_dict.get(k, 0) + one_dict[k]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69be4708-858b-49fd-82c6-9437d0cf463a",
   "metadata": {},
   "source": [
    "Sort the dictionary so the speakers with the most quotes appear first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df134db-4833-4b14-b179-9e8070e3bd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_combined_signi_dict = {k: v for k, v in sorted(combined_signi_dict.items(), key=lambda item: item[1], reverse=True)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5cf729-0697-4a88-94b3-8faf7cd737b1",
   "metadata": {},
   "source": [
    "And finally save the resulting dictionary into a file, this file can later be reused for multiple analyses, whenever we need to choose a representation of a group of people using the number of quotes to pick the most quoted individuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeabe2a1-4436-45b9-be62-df9fd8069e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_json_to_file('signi-quote-count-combined', sorted_combined_signi_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12018aae-3a5a-48a0-9e74-12f0a21763a9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Get the wikidata"
   ]
  },
  {
   "cell_type": "raw",
   "id": "507dbcdb-b7e6-4ba6-86d9-f663272730e8",
   "metadata": {},
   "source": [
    "SELECT DISTINCT ?item ?itemLabel \n",
    "          ?genderLabel ?citizenshipLabel ?languageLabel ?religionLabel ?ethnicLabel ?degreeLabel\n",
    "          ?dateOfBirth ?placeOfBirthLabel \n",
    "#           ?nativeNameLabel ?birthNameLabel ?givenNameLabel ?familyNameLabel ?pseudonymLabel \n",
    "#           ?fatherLabel ?motherLabel ?siblingLabel ?spouseLabel ?childLabel ?numOfChild \n",
    "#           ?occupationLabel ?positionLabel ?ideologyLabel ?educatedAtLabel\n",
    "          ?memberOfParty ?memberOfPartyLabel \n",
    "WHERE {\n",
    "  SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE]\". }\n",
    "  {\n",
    "    ?item p:P106 ?statement0.\n",
    "    ?statement0 (ps:P106) wd:Q82955.\n",
    "    {\n",
    "      ?item p:P102 ?statement1.\n",
    "      ?statement1 (ps:P102) wd:Q29552.\n",
    "    }\n",
    "    UNION\n",
    "    {\n",
    "      ?item p:P102 ?statement2.\n",
    "      ?statement2 (ps:P102) wd:Q29468.\n",
    "    }\n",
    "    MINUS {\n",
    "      ?item p:P570 ?statement_3.\n",
    "      ?statement_3 psv:P570 ?statementValue_3.\n",
    "      ?statementValue_3 wikibase:timePrecision ?precision_3.\n",
    "      FILTER(?precision_3 >= 11 )\n",
    "      ?statementValue_3 wikibase:timeValue ?P570_3.\n",
    "      FILTER(?P570_3 < \"+2015-01-01T00:00:00Z\"^^xsd:dateTime)\n",
    "    }\n",
    "    OPTIONAL { ?item wdt:P21 ?gender. }\n",
    "    OPTIONAL { ?item wdt:P27 ?citizenship. }\n",
    "    OPTIONAL { ?item wdt:P103 ?language. }\n",
    "    OPTIONAL { ?item wdt:P140 ?religion. }\n",
    "    OPTIONAL { ?item wdt:P172 ?ethnic. }\n",
    "    OPTIONAL { ?item wdt:P512 ?degree. }\n",
    "    \n",
    "    OPTIONAL { ?item wdt:P569 ?dateOfBirth. }\n",
    "    OPTIONAL { ?item wdt:P19 ?placeOfBirth. }\n",
    "    \n",
    "#     OPTIONAL { ?item wdt:P1559 ?nativeName. }\n",
    "#     OPTIONAL { ?item wdt:P1477 ?birthName. }\n",
    "#     OPTIONAL { ?item wdt:P735 ?givenName. }\n",
    "#     OPTIONAL { ?item wdt:P734 ?familyName. }\n",
    "#     OPTIONAL { ?item wdt:P742 ?pseudonym. }\n",
    "    \n",
    "#     OPTIONAL { ?item wdt:P22 ?father. }\n",
    "#     OPTIONAL { ?item wdt:P25 ?mother. }\n",
    "#     OPTIONAL { ?item wdt:P3373 ?sibling. }\n",
    "#     OPTIONAL { ?item wdt:P26 ?spouse. }\n",
    "#     OPTIONAL { ?item wdt:P40 ?child. }\n",
    "#     OPTIONAL { ?item wdt:P1971 ?numOfChild. }\n",
    "    \n",
    "#     OPTIONAL { ?item wdt:P106 ?occupation. }\n",
    "#     OPTIONAL { ?item wdt:P39 ?position. }\n",
    "#     OPTIONAL { ?item wdt:P1142 ?ideology. }\n",
    "#     OPTIONAL { ?item wdt:P69 ?educatedAt. }\n",
    "    \n",
    "    OPTIONAL { ?item wdt:P102 ?memberOfParty. }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d7475f-fb4d-4f46-9b9e-704f22e7631e",
   "metadata": {},
   "source": [
    "Merge duplicate objects representing a single speaker but with differing fields.\n",
    "<br>\n",
    "Example: Arnold Schwarzenegger has both Austrian and American nationalities, and would appear twice, once with Austrian, and once with American nationality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4fbda022-cad4-42ec-9715-18a6a7e346ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../quotebank/american_politicians_fixed.json\", \"r\") as f:\n",
    "    wiki_poli = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56b066b-505e-48a5-811f-de907c067193",
   "metadata": {},
   "outputs": [],
   "source": [
    "ampoli2 = dict()\n",
    "\n",
    "index = 0\n",
    "for row in wiki_poli:\n",
    "    item = row['item']\n",
    "    if item in ampoli2:\n",
    "        tmp = ampoli2[item]\n",
    "        columns = ['itemLabel', 'genderLabel', 'citizenshipLabel', 'religionLabel', 'ethnicLabel', 'degreeLabel', 'dateOfBirth', 'placeOfBirthLabel', 'memberOfParty', 'memberOfPartyLabel', 'languageLabel']\n",
    "        for col in columns:\n",
    "            if row.get(col, None) is None:\n",
    "                continue\n",
    "                \n",
    "            t = tmp.get(col, None)\n",
    "            \n",
    "            if t is None:\n",
    "                t = row[col]\n",
    "            elif isinstance(t, list):\n",
    "                if row[col] not in t:\n",
    "                    t.append(row[col])\n",
    "            elif row[col] != t:\n",
    "                t = [t, row[col]]\n",
    "                \n",
    "            tmp[col] = t\n",
    "    else:\n",
    "        ampoli2[item] = row"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
