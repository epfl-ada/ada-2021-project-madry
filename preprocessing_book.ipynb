{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfa19caa-6437-4f16-b6d5-01347159d330",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e447f2f-9f8e-4398-aaf6-21b33ef3ea41",
   "metadata": {},
   "source": [
    "The preprocessing pipeline should output a list of speakers with quotes. Each element of this list should contain information about the speaker (full name, gender, date of birth...) - obtained from wikidata, and a string formed by joining multiple quotes in order to get a string of fixed length."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80871718-bd1a-4806-b4be-c75f898ecc9a",
   "metadata": {},
   "source": [
    "The pipeline is presented through a data analysis example - analysing the personalities of the US politicians. In this example we take 100 politicians from both of the two major political parties, the Democratic party and the Republican party. We select the politicians which have the most quotes in our database. We only consider quotes for which the probability of the speaker is higher than 80% (referenced as significant quotes in this notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1bab713-bee5-46eb-a26f-446db88ac641",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bz2\n",
    "import json\n",
    "import re\n",
    "import random\n",
    "import sys\n",
    "import os\n",
    "import bz2\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d57b92-b15c-4f8e-8195-cfd3a7669877",
   "metadata": {},
   "source": [
    "### Counting significant quotes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08fdb05-f8e7-42fb-9f2b-2f00a60bde59",
   "metadata": {},
   "source": [
    "<b>This step has been executed once and will not be needed in following analyses since the quote counts calculated are for all the speakers and the output can be simply reused.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f263f01-5c14-48e6-8cfb-c6b813e17cb4",
   "metadata": {},
   "source": [
    "Define some methods for better reusability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4d02c4c6-18f0-4fbe-b387-a6fa98151145",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATTERN_INPUT = \"../quotebank/quotes-{}.json.bz2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90d36977-e339-4f1e-bfcd-85c0e38b86e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_json_to_file(name, obj):\n",
    "    # Use current timestamp to make the name of the file unique\n",
    "    millis = round(time.time() * 1000)\n",
    "    name = f'{name}_{millis}.json'\n",
    "    with open(name, 'wb') as f:\n",
    "        output = json.dumps(obj)\n",
    "        f.write(output.encode('utf-8'))\n",
    "    return name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51a096f-ec25-4b33-b006-dc2afc478a0a",
   "metadata": {},
   "source": [
    "Methods used for counting significant quotes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48807770-27e6-4cf5-b4b8-c6508f21db8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "signi_count = 0\n",
    "signi_quote_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0a00f07c-1f31-43fd-bb0a-701b9159b41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The signature remains from an older version of the code, parameter out_file could be removed, but then has to be removed in other places in the code as well.\n",
    "def initialize(out_file):\n",
    "    global signi_count\n",
    "    global signi_quote_dict\n",
    "    signi_count = 0\n",
    "    signi_quote_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c1b73b84-387f-4bf1-b7e3-46901c39ca9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The signature remains from an older version of the code, parameter out_file could be removed, but then has to be removed in other places in the code as well.\n",
    "def count_significant_quotes(out_file, row):\n",
    "    global signi_count\n",
    "    global signi_quote_dict\n",
    "    \n",
    "    probas = row['probas']\n",
    "    qids = row['qids']\n",
    "    \n",
    "    if (len(probas) == 0 or len(qids) == 0):\n",
    "        return\n",
    "    \n",
    "    if (probas[0][0] == 'None'):\n",
    "        return\n",
    "    \n",
    "    p = float(probas[0][1])\n",
    "    if (p < 0.8):\n",
    "        return\n",
    "    \n",
    "    qid = qids[0]\n",
    "    \n",
    "    signi_count = signi_count + 1\n",
    "    signi_quote_dict[qid] = signi_quote_dict.get(qid, 0) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d915fd-fc0e-4ed3-b3da-590afc1e309c",
   "metadata": {},
   "source": [
    "General methods used for processing the quotes files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5b902ef-f2a5-4eb0-b885-3c7d83326311",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Process a chunk of the input stream.\n",
    "\"\"\"\n",
    "def proc(input, evaluate_quote, max_length=20):\n",
    "    # Ugly global variable usage :(\n",
    "    global index\n",
    "    global invalid_json_count\n",
    "    global invalid_chunk_count\n",
    "    global chunk_stitching\n",
    "    global stitch_length\n",
    "    global scrap_next\n",
    "    global quote_is_open\n",
    "    global quote_part\n",
    "    global dat_part\n",
    "    global euro_error\n",
    "    global euro_count\n",
    "    \n",
    "    global totin\n",
    "    global totout\n",
    "    global prev\n",
    "    global dec\n",
    "    global start\n",
    "    \"\"\"Decompress and process a piece of a compressed stream\"\"\"\n",
    "    dat = dec.decompress(input)\n",
    "    got = len(dat)\n",
    "    if got != 0:    # 0 is common -- waiting for a bzip2 block\n",
    "        try:\n",
    "            if (euro_error):\n",
    "                # If the previous chunk ended unexpectedly and could not be decoded, try to combine it with this chunk\n",
    "                s = (dat_part + dat).decode('utf-8')\n",
    "                euro_error = False\n",
    "            else:\n",
    "                # Decode the current chunk\n",
    "                s = dat.decode('utf-8')\n",
    "                \n",
    "            # List elements in the quote files are separated by new lines (\\n)\n",
    "            lines = s.split('\\n')\n",
    "\n",
    "            for line in lines:\n",
    "                try:\n",
    "                    if (scrap_next):\n",
    "                        # If the object spans too many chunks we decide to scrap it, and keep scraping until JSON can parse the line (chunk)\n",
    "                        ob = json.loads(line)\n",
    "                        scrap_next = False\n",
    "                        quote_is_open = False\n",
    "                        chunk_stitching -= stitch_length\n",
    "                    else:\n",
    "                        if (quote_is_open):\n",
    "                            # If previous chunk ended in the middle of a JSON object we merge that content with the current line\n",
    "                            ob = json.loads(quote_part + line)\n",
    "                            quote_is_open = False\n",
    "                        else:\n",
    "                            # Parse the current line\n",
    "                            ob = json.loads(line)\n",
    "\n",
    "                    # Parametrization - do work on a single quote JSON object\n",
    "                    evaluate_quote({}, ob)\n",
    "                except ValueError:\n",
    "                    \"\"\"\n",
    "                    Error occurs when the line does not contain the whole JSON object, which happens for the last line in almost every chunk of input stream.\n",
    "                    We solve this by remembering the partial object, and then merging it with the rest of the object when we load the next chunk.\n",
    "                    JSON object might span more than 2 chunks, and in that case we keep merging until we reach max_length chunks, when we just throw away the object\n",
    "                    and count it as invalid using invalid_json_count.\n",
    "                    \"\"\"\n",
    "                    if (scrap_next):\n",
    "                        pass\n",
    "                    else:\n",
    "                        if (quote_is_open):\n",
    "                            chunk_stitching += 1\n",
    "                            quote_part = quote_part + line\n",
    "                            stitch_length += 1\n",
    "\n",
    "                            if (stitch_length > max_length):\n",
    "                                invalid_json_count += 1\n",
    "                                scrap_next = True\n",
    "                        else:\n",
    "                            quote_is_open = True\n",
    "                            quote_part = line\n",
    "                            stitch_length = 0\n",
    "        except UnicodeDecodeError as e:\n",
    "            # Error occurs when input stream is split in the middle of a character which is encoded with multiple bytes, for example the euro symbol\n",
    "            if (euro_error):\n",
    "                dat_part = dat_part + dat\n",
    "            else:\n",
    "                euro_error = True\n",
    "                dat_part = dat\n",
    "            \n",
    "            euro_count += 1\n",
    "        \n",
    "        index += 1\n",
    "    return got"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "cc51b57e-891d-419f-8f33-88b098afba19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_through_quotes(init, evaluate_quote, year, target_dict_name, path_to_input, name='test', chunk_size=16384):\n",
    "    global index\n",
    "    global invalid_json_count\n",
    "    global invalid_chunk_count\n",
    "    global chunk_stitching\n",
    "    global stitch_length\n",
    "    global scrap_next\n",
    "    global quote_is_open\n",
    "    global quote_part\n",
    "    global dat_part\n",
    "    global euro_error\n",
    "    global euro_count\n",
    "    \n",
    "    global totin\n",
    "    global totout\n",
    "    global prev\n",
    "    global dec\n",
    "    global start\n",
    "    \n",
    "    size = os.path.getsize(path_to_input)\n",
    "    invalid_json_count = 0\n",
    "    invalid_chunk_count = 0\n",
    "    chunk_stitching = 0\n",
    "    stitch_length = 0\n",
    "    scrap_next = False\n",
    "    quote_is_open = False\n",
    "    quote_part = ''\n",
    "    dat_part = 0\n",
    "    euro_error = False\n",
    "    euro_count = 0\n",
    "    \n",
    "    totin = 0\n",
    "    totout = 0\n",
    "    prev = -1\n",
    "    dec = bz2.BZ2Decompressor()\n",
    "    start = time.time()\n",
    "    \n",
    "    init({})\n",
    "    \n",
    "    target_dict = poli_quotes if target_dict_name == \"poli_quotes\" else signi_quote_dict\n",
    "    index = 0\n",
    "    with open(path_to_input, 'rb') as f:\n",
    "        for chunk in iter(lambda: f.read(chunk_size), b''):\n",
    "            # feed chunk to decompressor\n",
    "            got = proc(chunk, evaluate_quote)\n",
    "\n",
    "            # handle case of concatenated bz2 streams\n",
    "            if dec.eof:\n",
    "                rem = dec.unused_data\n",
    "                dec = bz2.BZ2Decompressor()\n",
    "                got += proc(rem, evaluate_quote)\n",
    "\n",
    "            # show progress\n",
    "            totin += len(chunk)\n",
    "            totout += got\n",
    "            if got != 0:    # only if a bzip2 block emitted\n",
    "                frac = round(1000 * totin / size)\n",
    "                if frac != prev:\n",
    "                    left = (size / totin - 1) * (time.time() - start)\n",
    "                    print(f'\\r{frac / 10:.1f}% (~{left:.1f}s left)\\tyear: {year}\\tnumber of speakers: {len(target_dict)}\\tstitching: {chunk_stitching}\\teuro count: {euro_count}\\tinvalid json count: {invalid_json_count}\\tinvalid chunk count: {invalid_chunk_count}', end='')\n",
    "                    prev = frac\n",
    "\n",
    "    # Show the resulting size.\n",
    "    print(end='\\r')\n",
    "    print(totout, 'uncompressed bytes')\n",
    "\n",
    "    output_name = write_json_to_file(f'{name}-{year}', target_dict)\n",
    "    return output_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7328f060-7f5b-49d3-94b8-a9633427bd5e",
   "metadata": {},
   "source": [
    "Create files for every year, each file contains a dictionary where the key is the QID of the speaker, and the value is the number of significant quotes.\n",
    "<br><br>\n",
    "<font color='red'>WARNING: LONG EXECUTION!</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d8f659-4c51-4fb1-9baa-5e7bbf86fca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "years = [2015, 2016, 2017, 2018, 2019, 2020]\n",
    "# years = [2020]\n",
    "for year in years:\n",
    "    path_to_input = PATTERN_INPUT.format(year)\n",
    "    \n",
    "    run_through_quotes(\n",
    "        initialize, count_significant_quotes, year, \"signi_quote_dict\", path_to_input, name='signi-quote-count', chunk_size=1_048_576)\n",
    "    print('')\n",
    "    print(f'Finished counting quotes for the year {year}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11b13ee-41a4-4d9e-969b-ee949ebe30f9",
   "metadata": {},
   "source": [
    "Now combine the quote counts into a single file.\n",
    "<br>\n",
    "An example of the file names is used, the string should be updated if the code is ran again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "88727e52-19fb-447a-b0a4-653582d13d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "signi_quotes_file_names = [\n",
    "    \"signi-quote-count-2015_1636244638891.json\",\n",
    "    \"signi-quote-count-2016_1636246832187.json\",\n",
    "    \"signi-quote-count-2017_1636249273913.json\",\n",
    "    \"signi-quote-count-2018_1636250518608.json\",\n",
    "    \"signi-quote-count-2019_1636251729971.json\",\n",
    "    \"signi-quote-count-2020_1636237785105.json\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "95b0d00c-0663-476f-ad5d-f6afd69dee24",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_signi_dict = {}\n",
    "\n",
    "for file_name in signi_quotes_file_names:\n",
    "    with open(file_name, 'r') as f:\n",
    "        one_dict = json.load(f)\n",
    "        for k in one_dict.keys():\n",
    "            combined_signi_dict[k] = combined_signi_dict.get(k, 0) + one_dict[k]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69be4708-858b-49fd-82c6-9437d0cf463a",
   "metadata": {},
   "source": [
    "Sort the dictionary so the speakers with the most quotes appear first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9df134db-4833-4b14-b179-9e8070e3bd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_combined_signi_dict = {k: v for k, v in sorted(combined_signi_dict.items(), key=lambda item: item[1], reverse=True)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5cf729-0697-4a88-94b3-8faf7cd737b1",
   "metadata": {},
   "source": [
    "And finally save the resulting dictionary into a file, this file can later be reused for multiple analyses, whenever we need to choose a representation of a group of people using the number of quotes to pick the most quoted individuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeabe2a1-4436-45b9-be62-df9fd8069e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_json_to_file('signi-quote-count-combined', sorted_combined_signi_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12018aae-3a5a-48a0-9e74-12f0a21763a9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Get the wikidata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f674fda1-e30d-4499-8588-6d1e1092fbe3",
   "metadata": {},
   "source": [
    "We used the https://query.wikidata.org/ website to get the relevant wikidata. The SPARQL query is in the following cell.\n",
    "<br>\n",
    "We can do this (and did do afterwards) using the provided wikidata parquet file as well."
   ]
  },
  {
   "cell_type": "raw",
   "id": "507dbcdb-b7e6-4ba6-86d9-f663272730e8",
   "metadata": {},
   "source": [
    "SELECT DISTINCT ?item ?itemLabel \n",
    "          ?genderLabel ?citizenshipLabel ?languageLabel ?religionLabel ?ethnicLabel ?degreeLabel\n",
    "          ?dateOfBirth ?placeOfBirthLabel \n",
    "#           ?nativeNameLabel ?birthNameLabel ?givenNameLabel ?familyNameLabel ?pseudonymLabel \n",
    "#           ?fatherLabel ?motherLabel ?siblingLabel ?spouseLabel ?childLabel ?numOfChild \n",
    "#           ?occupationLabel ?positionLabel ?ideologyLabel ?educatedAtLabel\n",
    "          ?memberOfParty ?memberOfPartyLabel \n",
    "WHERE {\n",
    "  SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE]\". }\n",
    "  {\n",
    "    ?item p:P106 ?statement0.\n",
    "    ?statement0 (ps:P106) wd:Q82955.\n",
    "    {\n",
    "      ?item p:P102 ?statement1.\n",
    "      ?statement1 (ps:P102) wd:Q29552.\n",
    "    }\n",
    "    UNION\n",
    "    {\n",
    "      ?item p:P102 ?statement2.\n",
    "      ?statement2 (ps:P102) wd:Q29468.\n",
    "    }\n",
    "    MINUS {\n",
    "      ?item p:P570 ?statement_3.\n",
    "      ?statement_3 psv:P570 ?statementValue_3.\n",
    "      ?statementValue_3 wikibase:timePrecision ?precision_3.\n",
    "      FILTER(?precision_3 >= 11 )\n",
    "      ?statementValue_3 wikibase:timeValue ?P570_3.\n",
    "      FILTER(?P570_3 < \"+2015-01-01T00:00:00Z\"^^xsd:dateTime)\n",
    "    }\n",
    "    OPTIONAL { ?item wdt:P21 ?gender. }\n",
    "    OPTIONAL { ?item wdt:P27 ?citizenship. }\n",
    "    OPTIONAL { ?item wdt:P103 ?language. }\n",
    "    OPTIONAL { ?item wdt:P140 ?religion. }\n",
    "    OPTIONAL { ?item wdt:P172 ?ethnic. }\n",
    "    OPTIONAL { ?item wdt:P512 ?degree. }\n",
    "    \n",
    "    OPTIONAL { ?item wdt:P569 ?dateOfBirth. }\n",
    "    OPTIONAL { ?item wdt:P19 ?placeOfBirth. }\n",
    "    \n",
    "#     OPTIONAL { ?item wdt:P1559 ?nativeName. }\n",
    "#     OPTIONAL { ?item wdt:P1477 ?birthName. }\n",
    "#     OPTIONAL { ?item wdt:P735 ?givenName. }\n",
    "#     OPTIONAL { ?item wdt:P734 ?familyName. }\n",
    "#     OPTIONAL { ?item wdt:P742 ?pseudonym. }\n",
    "    \n",
    "#     OPTIONAL { ?item wdt:P22 ?father. }\n",
    "#     OPTIONAL { ?item wdt:P25 ?mother. }\n",
    "#     OPTIONAL { ?item wdt:P3373 ?sibling. }\n",
    "#     OPTIONAL { ?item wdt:P26 ?spouse. }\n",
    "#     OPTIONAL { ?item wdt:P40 ?child. }\n",
    "#     OPTIONAL { ?item wdt:P1971 ?numOfChild. }\n",
    "    \n",
    "#     OPTIONAL { ?item wdt:P106 ?occupation. }\n",
    "#     OPTIONAL { ?item wdt:P39 ?position. }\n",
    "#     OPTIONAL { ?item wdt:P1142 ?ideology. }\n",
    "#     OPTIONAL { ?item wdt:P69 ?educatedAt. }\n",
    "    \n",
    "    OPTIONAL { ?item wdt:P102 ?memberOfParty. }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d7475f-fb4d-4f46-9b9e-704f22e7631e",
   "metadata": {},
   "source": [
    "Merge duplicate objects representing a single speaker but with differing fields.\n",
    "<br>\n",
    "Example: Arnold Schwarzenegger has both Austrian and American nationalities, and would appear twice, once with Austrian, and once with American nationality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4fbda022-cad4-42ec-9715-18a6a7e346ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../quotebank/american_politicians_fixed.json\", \"r\") as f:\n",
    "    wiki_poli = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c56b066b-505e-48a5-811f-de907c067193",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_poli_merged = dict()\n",
    "\n",
    "index = 0\n",
    "for row in wiki_poli:\n",
    "    # Extract the QID from the link (ex. http://www.wikidata.org/entity/Q203286 -> Q203286)\n",
    "    qid_start = row['item'].rindex('/') + 1\n",
    "    key = row['item'][qid_start:]\n",
    "    # Replace the link with the QID\n",
    "    row['item'] = key\n",
    "    \n",
    "    if key in wiki_poli_merged:\n",
    "        merged_entry = wiki_poli_merged[key]\n",
    "        columns = ['itemLabel', 'genderLabel', 'citizenshipLabel', 'religionLabel', 'ethnicLabel', 'degreeLabel', 'dateOfBirth', 'placeOfBirthLabel', 'memberOfParty', 'memberOfPartyLabel', 'languageLabel']\n",
    "        \"\"\"\n",
    "        Merge the values for every column:\n",
    "            - if the values are the same - do nothing\n",
    "            - if the values are different - create a list and add them both\n",
    "        \"\"\"\n",
    "        for col in columns:\n",
    "            if row.get(col, None) is None:\n",
    "                continue\n",
    "                \n",
    "            updated_entry = merged_entry.get(col, None)\n",
    "            \n",
    "            if updated_entry is None:\n",
    "                updated_entry = row[col]\n",
    "            elif isinstance(updated_entry, list):\n",
    "                if row[col] not in updated_entry:\n",
    "                    updated_entry.append(row[col])\n",
    "            elif row[col] != updated_entry:\n",
    "                updated_entry = [updated_entry, row[col]]\n",
    "                \n",
    "            merged_entry[col] = updated_entry\n",
    "    else:\n",
    "        wiki_poli_merged[key] = row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b884918a-8b0f-46bf-a9fc-6c28a0f48645",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'american_politicians_final_1636570708023.json'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "write_json_to_file('american_politicians_final', wiki_poli_merged)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3eb4e4a-00d7-461d-a6ee-0fcc4d85d707",
   "metadata": {},
   "source": [
    "### Get the 100 most quoted party members"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c17cd2e-0646-4175-971f-68f6f3e24d90",
   "metadata": {},
   "source": [
    "Using the results of the previous two steps - the number of quotes for each speaker, and the list of US politicians, we can compile a list of 100 most quoted members of the two major US political parties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2e228fe3-ebfd-4803-b998-54c2be188e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_list = []\n",
    "rep_list = []\n",
    "\n",
    "CAP_TARGET = 100\n",
    "DEM_PARTY = \"http://www.wikidata.org/entity/Q29552\"\n",
    "REP_PARTY = \"http://www.wikidata.org/entity/Q29468\"\n",
    "\n",
    "for v in sorted_combined_signi_dict:\n",
    "    row = wiki_poli_merged.get(v, None)\n",
    "    \n",
    "    # Could not find person in the politician dictionary\n",
    "    if row is None:\n",
    "        continue\n",
    "    \n",
    "    memberOfParty = row.get('memberOfParty', None)\n",
    "    if memberOfParty is None:\n",
    "        continue\n",
    "    \n",
    "    # Cast to one element list if not already a list\n",
    "    if isinstance(memberOfParty, list) == False:\n",
    "        memberOfParty = [memberOfParty]\n",
    "    \n",
    "    # Check membership\n",
    "    if DEM_PARTY in memberOfParty:\n",
    "        if REP_PARTY in memberOfParty:\n",
    "            # member of both parties, just skip\n",
    "            continue\n",
    "\n",
    "        # Check if the list is already at full capacity\n",
    "        if len(dem_list) < CAP_TARGET:\n",
    "            dem_list.append(row)\n",
    "    elif REP_PARTY in memberOfParty:\n",
    "        # Check if the list is already at full capacity\n",
    "        if len(rep_list) < CAP_TARGET:\n",
    "            rep_list.append(row)\n",
    "    \n",
    "    # Check if both lists are at full capacity\n",
    "    if len(dem_list) == CAP_TARGET and len(rep_list) == CAP_TARGET:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3f8832-5908-449a-aa39-f54b5e35e629",
   "metadata": {},
   "source": [
    "### Get the politician quotes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144fa20f-4433-498f-bd8b-bc2d9580293a",
   "metadata": {},
   "source": [
    "For the politicians in the previously compiled lists, we now fetch the quotes from the quote files. We use the methods defined at the top of this notebook, which were written in a reusable way."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9618f4-4d12-4d48-921e-403c88fee32b",
   "metadata": {},
   "source": [
    "Define the initialization and visit methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a58f1a2d-722c-49a9-9bb5-a71e01c1049e",
   "metadata": {},
   "outputs": [],
   "source": [
    "poli_quotes = {}\n",
    "poli_people = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "1b22d807-3800-4807-b6c3-01454504267e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def poli_initialize(out_file):\n",
    "    global poli_quotes\n",
    "    global poli_people\n",
    "    global dem_list\n",
    "    global rep_list\n",
    "    \n",
    "    poli_quotes = {}\n",
    "    poli_people = set()\n",
    "    \n",
    "    for v in dem_list:\n",
    "        poli_people.add(v['item'])\n",
    "    for v in rep_list:\n",
    "        poli_people.add(v['item'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a3cb0a9b-3e5a-4fd4-bccc-a005c4dde7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Remember the quote, only if it belongs to one of the politicians in the set poli_people, and if the probability is over 80%.\n",
    "\"\"\"\n",
    "def save_politician_quotes(out_file, row):\n",
    "    global poli_quotes\n",
    "    global poli_people\n",
    "    \n",
    "    probas = row['probas']\n",
    "    qids = row['qids']\n",
    "    \n",
    "    # Check if the probability field exists\n",
    "    if (len(probas) == 0 or len(qids) == 0):\n",
    "        return\n",
    "    \n",
    "    if (probas[0][0] == 'None'):\n",
    "        return\n",
    "    \n",
    "    # Check if the probability is over 80%\n",
    "    p = float(probas[0][1])\n",
    "    if (p < 0.8):\n",
    "        return\n",
    "    \n",
    "    # Check if the speaker is one of the 100 party members\n",
    "    qid = qids[0]\n",
    "    if qid not in poli_people:\n",
    "        return\n",
    "    \n",
    "    # Remember only the quote and the probability\n",
    "    data = {}\n",
    "    data['quotation'] = row['quotation']\n",
    "    data['proba'] = row['probas'][0][1]\n",
    "    \n",
    "    # Append the quote\n",
    "    arr = poli_quotes.get(qid, [])\n",
    "    arr.append(data)\n",
    "    poli_quotes[qid] = arr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee80d1f-1b4d-4b12-b480-b98be3534c4b",
   "metadata": {},
   "source": [
    "Create files for every year, each file contains a dictionary where the key is the QID of the speaker, and the value is the list of significant quotes attributed to the speaker.\n",
    "<br><br>\n",
    "<font color='red'>WARNING: LONG EXECUTION!</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d43542e-879b-4e5b-a583-1b78db048da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "years = [2015, 2016, 2017, 2018, 2019, 2020]\n",
    "# years = [2020]\n",
    "for year in years:\n",
    "    path_to_input = PATTERN_INPUT.format(year)\n",
    "    \n",
    "    run_through_quotes(\n",
    "        poli_initialize, save_politician_quotes, year, \"poli_quotes\", path_to_input, name='politician-quotes', chunk_size=1_048_576)\n",
    "    print('')\n",
    "    print(f'Finished compiling quotes for the year {year}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7ffd50-cad0-4be1-81f5-dae693471721",
   "metadata": {},
   "source": [
    "### Combine the quotes and the wikidata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30180476-46c2-400d-a424-e9f7a0757a56",
   "metadata": {},
   "source": [
    "Now we combine the politician quotes with their wikidata information. We use the 6 files of politician quotes created in the previous step, as well as the list of the party members. The result is a file which contains 200 entries, where each entry represents one politician, and contains their wikidata info as well as a list of quotes. The list of quotes can be quite long for some of the politicians."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "ba1bacad-9fb3-407f-b6ba-1a4422135c2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'politician-quotes-combined_1636575214462.json'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poli_quote_files = [\n",
    "    \"../quotebank/politician-quotes-2015_1636331534906.json\",\n",
    "    \"../quotebank/politician-quotes-2016_1636332058163.json\",\n",
    "    \"../quotebank/politician-quotes-2017_1636333168732.json\",\n",
    "    \"../quotebank/politician-quotes-2018_1636334221167.json\",\n",
    "    \"../quotebank/politician-quotes-2019_1636335010497.json\",\n",
    "    \"../quotebank/politician-quotes-2020_1636330658142.json\"\n",
    "]\n",
    "\n",
    "poli_quotes_combined = {}\n",
    "\n",
    "both_parties = dem_list + rep_list\n",
    "for v in both_parties:\n",
    "    copy = dict(v)\n",
    "    copy['quotations'] = []\n",
    "    \n",
    "    poli_quotes_combined[v['item']] = copy\n",
    "\n",
    "for poli_quote_file_name in poli_quote_files:\n",
    "    with open(poli_quote_file_name, 'r') as f:\n",
    "        quotes = json.load(f)\n",
    "        \n",
    "        for k in quotes.keys():\n",
    "            poli_quotes_combined[k]['quotations'] += quotes[k]\n",
    "\n",
    "write_json_to_file('politician-quotes-combined', poli_quotes_combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3147ad-1297-4bf9-a60b-fde7fe4bb168",
   "metadata": {},
   "source": [
    "### Filter the quotes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993e2f8a-7e2d-45f9-b283-db99cceb3db2",
   "metadata": {},
   "source": [
    "Some of the quotes in the database do not represent actual quotes, but instead contain junk like html tags, source code, or text from the webpage where the source article was published.\n",
    "<br>\n",
    "We filter these quotes out so our dataset is not polluted by junk data. We have found a few filters which detect most of the junk data, while maintaining a low false positive rate:\n",
    "<ul>\n",
    "    <li>quotes which contains very long 'words' - more than 50 characters</li>\n",
    "    <li>quotes which contain URLs - these usually contain other junk characters</li>\n",
    "    <li>quotes which contains JSON-like key-value pairs</li>\n",
    "    <li>quotes which contain a lot of special characters (more than 10% of total characters)</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "e49ed80f-0a0d-4ead-9beb-8b5447e527fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../quotebank/politician-quotes-combined_1636336204264.json', 'r') as f:\n",
    "    poli_quotes_filtered = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "ca6885b3-4f20-46f4-96c5-0eb4c6ef923e",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_quotes = []\n",
    "\n",
    "weird_pattern = '[_@#+&;:\\(\\)\\{\\}\\[\\]\\\\/`]'\n",
    "json_pattern = '\\{.*[a-zA-Z]+:\\s[\\'\"`][a-zA-Z0-9]+[\\'\"`].*\\}'\n",
    "url_pattern = 'https?'\n",
    "\n",
    "for k in poli_quotes_filtered.keys():\n",
    "    elem = poli_quotes_filtered[k]\n",
    "    \n",
    "    new_arr = []\n",
    "    for entry in elem['quotations']:\n",
    "        text = entry['quotation']\n",
    "        \n",
    "        longest = max(entry['quotation'].split(), key=len)\n",
    "        if (len(longest) > 50):\n",
    "            filtered_quotes.append(entry)\n",
    "            continue\n",
    "        \n",
    "        if re.search(url_pattern, text) is not None:\n",
    "            filtered_quotes.append(entry)\n",
    "            continue\n",
    "        \n",
    "        if re.search(json_pattern, text) is not None:\n",
    "            filtered_quotes.append(entry)\n",
    "            continue\n",
    "            \n",
    "        weird_num = len(re.findall(weird_pattern, text))\n",
    "        total = len(text)\n",
    "        weird_percent = weird_num / total\n",
    "        if (weird_percent > 0.1):\n",
    "            filtered_quotes.append(entry)\n",
    "            continue\n",
    "            \n",
    "        new_arr.append(entry)\n",
    "    elem['quotations'] = new_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "8a3ed2bb-4806-452b-931d-df96cc058994",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/politician-quotes-combined-and-filtered_1636577222699.json'"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "write_json_to_file('data/politician-quotes-combined-and-filtered', poli_quotes_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29ea8c3-b33e-4988-a18c-c26c871cd41b",
   "metadata": {},
   "source": [
    "### Concatenate the quotes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c179925a-2e23-4a98-91e0-1c5845426efc",
   "metadata": {},
   "source": [
    "Finally, we concatenate the quotes into a single fixed-length string. We do this because of the limitation of the CSV file format, which can contains at most ~32000 characters in a single field. This means that most of the quotes will not be used.\n",
    "<br>\n",
    "Alternatively, we could use multiple fields for the same speaker, but we think the amount of characters that can fit in a single cell is enough for a decent analysis.\n",
    "<br>\n",
    "We sort the quotes by length and use the longest ones first. We do this because the longer quotes are a better representation of a person's speach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "4156eae2-9288-477d-bdb9-258de315a476",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/politician-quotes-combined-and-filtered_1636577222699.json', 'r') as f:\n",
    "    poli_quotes_concat = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "a02ee34d-512c-4ce3-8fbf-1a52ad915c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUOTE_LENGTH = 5000\n",
    "\n",
    "for k in poli_quotes_concat.keys():\n",
    "    elem = poli_quotes_concat[k]\n",
    "    \n",
    "    # Sort the quotes by length\n",
    "    elem['quotations'].sort(key = lambda x: len(x['quotation']), reverse = True)\n",
    "    \n",
    "    concat = ''\n",
    "    for quote in elem['quotations']:\n",
    "        # Concatenate the quotes\n",
    "        concat += ' ' + quote['quotation']\n",
    "        \n",
    "        # Trim if we are over QUOTE_LENGTH\n",
    "        if (len(concat) >= QUOTE_LENGTH):\n",
    "            concat = concat[0:QUOTE_LENGTH]\n",
    "            break\n",
    "    \n",
    "    elem['quotations'] = concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d600af-93ec-400f-bbf1-b9442ca1bd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_dict_to_file('data/politician-quotes-concatenated', poli_quotes_concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbe3383-769e-4af8-a31b-f04ad3454796",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
