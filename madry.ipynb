{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfa19caa-6437-4f16-b6d5-01347159d330",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e447f2f-9f8e-4398-aaf6-21b33ef3ea41",
   "metadata": {},
   "source": [
    "The preprocessing pipeline should output a list of speakers with quotes. Each element of this list should contain information about the speaker (full name, gender, date of birth...) - obtained from wikidata, and a string formed by joining multiple quotes in order to get a string of fixed length."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80871718-bd1a-4806-b4be-c75f898ecc9a",
   "metadata": {},
   "source": [
    "The pipeline is presented through a data analysis example - analysing the personalities of the US politicians. In this example we take 100 politicians from both of the two major political parties, the Democratic party and the Republican party. We select the politicians which have the most quotes in our database. We only consider quotes for which the probability of the speaker is higher than 80% (referenced as significant quotes in this notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1bab713-bee5-46eb-a26f-446db88ac641",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bz2\n",
    "import json\n",
    "import re\n",
    "import random\n",
    "import sys\n",
    "import os\n",
    "import bz2\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d57b92-b15c-4f8e-8195-cfd3a7669877",
   "metadata": {},
   "source": [
    "### Counting significant quotes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08fdb05-f8e7-42fb-9f2b-2f00a60bde59",
   "metadata": {},
   "source": [
    "<b>This step has been executed once and will not be needed in following analyses since the quote counts calculated are for all the speakers and the output can be simply reused.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f263f01-5c14-48e6-8cfb-c6b813e17cb4",
   "metadata": {},
   "source": [
    "Define some methods for better reusability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4d02c4c6-18f0-4fbe-b387-a6fa98151145",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATTERN_INPUT = \"../quotebank/quotes-{}.json.bz2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90d36977-e339-4f1e-bfcd-85c0e38b86e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_json_to_file(name, obj):\n",
    "    # Use current timestamp to make the name of the file unique\n",
    "    millis = round(time.time() * 1000)\n",
    "    name = f'{name}_{millis}.json'\n",
    "    with open(name, 'wb') as f:\n",
    "        output = json.dumps(obj)\n",
    "        f.write(output.encode('utf-8'))\n",
    "    return name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51a096f-ec25-4b33-b006-dc2afc478a0a",
   "metadata": {},
   "source": [
    "Methods used for counting significant quotes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48807770-27e6-4cf5-b4b8-c6508f21db8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "signi_count = 0\n",
    "signi_quote_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0a00f07c-1f31-43fd-bb0a-701b9159b41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The signature remains from an older version of the code, parameter out_file could be removed, but then has to be removed in other places in the code as well.\n",
    "def initialize(out_file):\n",
    "    global signi_count\n",
    "    global signi_quote_dict\n",
    "    signi_count = 0\n",
    "    signi_quote_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c1b73b84-387f-4bf1-b7e3-46901c39ca9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The signature remains from an older version of the code, parameter out_file could be removed, but then has to be removed in other places in the code as well.\n",
    "def count_significant_quotes(out_file, row):\n",
    "    global signi_count\n",
    "    global signi_quote_dict\n",
    "    \n",
    "    probas = row['probas']\n",
    "    qids = row['qids']\n",
    "    \n",
    "    if (len(probas) == 0 or len(qids) == 0):\n",
    "        return\n",
    "    \n",
    "    if (probas[0][0] == 'None'):\n",
    "        return\n",
    "    \n",
    "    p = float(probas[0][1])\n",
    "    if (p < 0.8):\n",
    "        return\n",
    "    \n",
    "    qid = qids[0]\n",
    "    \n",
    "    signi_count = signi_count + 1\n",
    "    signi_quote_dict[qid] = signi_quote_dict.get(qid, 0) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d915fd-fc0e-4ed3-b3da-590afc1e309c",
   "metadata": {},
   "source": [
    "General methods used for processing the quotes files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5b902ef-f2a5-4eb0-b885-3c7d83326311",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Process a chunk of the input stream.\n",
    "\"\"\"\n",
    "def proc(input, evaluate_quote, max_length=20):\n",
    "    # Ugly global variable usage :(\n",
    "    global index\n",
    "    global invalid_json_count\n",
    "    global invalid_chunk_count\n",
    "    global chunk_stitching\n",
    "    global stitch_length\n",
    "    global scrap_next\n",
    "    global quote_is_open\n",
    "    global quote_part\n",
    "    global dat_part\n",
    "    global euro_error\n",
    "    global euro_count\n",
    "    \n",
    "    global totin\n",
    "    global totout\n",
    "    global prev\n",
    "    global dec\n",
    "    global start\n",
    "    \"\"\"Decompress and process a piece of a compressed stream\"\"\"\n",
    "    dat = dec.decompress(input)\n",
    "    got = len(dat)\n",
    "    if got != 0:    # 0 is common -- waiting for a bzip2 block\n",
    "        try:\n",
    "            if (euro_error):\n",
    "                # If the previous chunk ended unexpectedly and could not be decoded, try to combine it with this chunk\n",
    "                s = (dat_part + dat).decode('utf-8')\n",
    "                euro_error = False\n",
    "            else:\n",
    "                # Decode the current chunk\n",
    "                s = dat.decode('utf-8')\n",
    "                \n",
    "            # List elements in the quote files are separated by new lines (\\n)\n",
    "            lines = s.split('\\n')\n",
    "\n",
    "            for line in lines:\n",
    "                try:\n",
    "                    if (scrap_next):\n",
    "                        # If the object spans too many chunks we decide to scrap it, and keep scraping until JSON can parse the line (chunk)\n",
    "                        ob = json.loads(line)\n",
    "                        scrap_next = False\n",
    "                        quote_is_open = False\n",
    "                        chunk_stitching -= stitch_length\n",
    "                    else:\n",
    "                        if (quote_is_open):\n",
    "                            # If previous chunk ended in the middle of a JSON object we merge that content with the current line\n",
    "                            ob = json.loads(quote_part + line)\n",
    "                            quote_is_open = False\n",
    "                        else:\n",
    "                            # Parse the current line\n",
    "                            ob = json.loads(line)\n",
    "\n",
    "                    # Parametrization - do work on a single quote JSON object\n",
    "                    evaluate_quote({}, ob)\n",
    "                except ValueError:\n",
    "                    \"\"\"\n",
    "                    Error occurs when the line does not contain the whole JSON object, which happens for the last line in almost every chunk of input stream.\n",
    "                    We solve this by remembering the partial object, and then merging it with the rest of the object when we load the next chunk.\n",
    "                    JSON object might span more than 2 chunks, and in that case we keep merging until we reach max_length chunks, when we just throw away the object\n",
    "                    and count it as invalid using invalid_json_count.\n",
    "                    \"\"\"\n",
    "                    if (scrap_next):\n",
    "                        pass\n",
    "                    else:\n",
    "                        if (quote_is_open):\n",
    "                            chunk_stitching += 1\n",
    "                            quote_part = quote_part + line\n",
    "                            stitch_length += 1\n",
    "\n",
    "                            if (stitch_length > max_length):\n",
    "                                invalid_json_count += 1\n",
    "                                scrap_next = True\n",
    "                        else:\n",
    "                            quote_is_open = True\n",
    "                            quote_part = line\n",
    "                            stitch_length = 0\n",
    "        except UnicodeDecodeError as e:\n",
    "            # Error occurs when input stream is split in the middle of a character which is encoded with multiple bytes, for example the euro symbol\n",
    "            if (euro_error):\n",
    "                dat_part = dat_part + dat\n",
    "            else:\n",
    "                euro_error = True\n",
    "                dat_part = dat\n",
    "            \n",
    "            euro_count += 1\n",
    "        \n",
    "        index += 1\n",
    "    return got"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "cc51b57e-891d-419f-8f33-88b098afba19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_through_quotes(init, evaluate_quote, year, target_dict_name, path_to_input, name='test', chunk_size=16384):\n",
    "    global index\n",
    "    global invalid_json_count\n",
    "    global invalid_chunk_count\n",
    "    global chunk_stitching\n",
    "    global stitch_length\n",
    "    global scrap_next\n",
    "    global quote_is_open\n",
    "    global quote_part\n",
    "    global dat_part\n",
    "    global euro_error\n",
    "    global euro_count\n",
    "    \n",
    "    global totin\n",
    "    global totout\n",
    "    global prev\n",
    "    global dec\n",
    "    global start\n",
    "    \n",
    "    size = os.path.getsize(path_to_input)\n",
    "    invalid_json_count = 0\n",
    "    invalid_chunk_count = 0\n",
    "    chunk_stitching = 0\n",
    "    stitch_length = 0\n",
    "    scrap_next = False\n",
    "    quote_is_open = False\n",
    "    quote_part = ''\n",
    "    dat_part = 0\n",
    "    euro_error = False\n",
    "    euro_count = 0\n",
    "    \n",
    "    totin = 0\n",
    "    totout = 0\n",
    "    prev = -1\n",
    "    dec = bz2.BZ2Decompressor()\n",
    "    start = time.time()\n",
    "    \n",
    "    init({})\n",
    "    \n",
    "    target_dict = poli_quotes if target_dict_name == \"poli_quotes\" else signi_quote_dict\n",
    "    index = 0\n",
    "    with open(path_to_input, 'rb') as f:\n",
    "        for chunk in iter(lambda: f.read(chunk_size), b''):\n",
    "            # feed chunk to decompressor\n",
    "            got = proc(chunk, evaluate_quote)\n",
    "\n",
    "            # handle case of concatenated bz2 streams\n",
    "            if dec.eof:\n",
    "                rem = dec.unused_data\n",
    "                dec = bz2.BZ2Decompressor()\n",
    "                got += proc(rem, evaluate_quote)\n",
    "\n",
    "            # show progress\n",
    "            totin += len(chunk)\n",
    "            totout += got\n",
    "            if got != 0:    # only if a bzip2 block emitted\n",
    "                frac = round(1000 * totin / size)\n",
    "                if frac != prev:\n",
    "                    left = (size / totin - 1) * (time.time() - start)\n",
    "                    print(f'\\r{frac / 10:.1f}% (~{left:.1f}s left)\\tyear: {year}\\tnumber of speakers: {len(target_dict)}\\tstitching: {chunk_stitching}\\teuro count: {euro_count}\\tinvalid json count: {invalid_json_count}\\tinvalid chunk count: {invalid_chunk_count}', end='')\n",
    "                    prev = frac\n",
    "\n",
    "    # Show the resulting size.\n",
    "    print(end='\\r')\n",
    "    print(totout, 'uncompressed bytes')\n",
    "\n",
    "    output_name = write_json_to_file(f'{name}-{year}', target_dict)\n",
    "    return output_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7328f060-7f5b-49d3-94b8-a9633427bd5e",
   "metadata": {},
   "source": [
    "Create files for every year, each file contains a dictionary where the key is the QID of the speaker, and the value is the number of significant quotes.\n",
    "<br><br>\n",
    "<font color='red'>WARNING: LONG EXECUTION!</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d8f659-4c51-4fb1-9baa-5e7bbf86fca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "years = [2015, 2016, 2017, 2018, 2019, 2020]\n",
    "# years = [2020]\n",
    "for year in years:\n",
    "    path_to_input = PATTERN_INPUT.format(year)\n",
    "    \n",
    "    run_through_quotes(\n",
    "        initialize, count_significant_quotes, year, \"signi_quote_dict\", path_to_input, name='signi-quote-count', chunk_size=1_048_576)\n",
    "    print('')\n",
    "    print(f'Finished counting quotes for the year {year}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11b13ee-41a4-4d9e-969b-ee949ebe30f9",
   "metadata": {},
   "source": [
    "Now combine the quote counts into a single file.\n",
    "<br>\n",
    "An example of the file names is used, the string should be updated if the code is ran again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "88727e52-19fb-447a-b0a4-653582d13d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "signi_quotes_file_names = [\n",
    "    \"signi-quote-count-2015_1636244638891.json\",\n",
    "    \"signi-quote-count-2016_1636246832187.json\",\n",
    "    \"signi-quote-count-2017_1636249273913.json\",\n",
    "    \"signi-quote-count-2018_1636250518608.json\",\n",
    "    \"signi-quote-count-2019_1636251729971.json\",\n",
    "    \"signi-quote-count-2020_1636237785105.json\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "95b0d00c-0663-476f-ad5d-f6afd69dee24",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_signi_dict = {}\n",
    "\n",
    "for file_name in signi_quotes_file_names:\n",
    "    with open(file_name, 'r') as f:\n",
    "        one_dict = json.load(f)\n",
    "        for k in one_dict.keys():\n",
    "            combined_signi_dict[k] = combined_signi_dict.get(k, 0) + one_dict[k]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69be4708-858b-49fd-82c6-9437d0cf463a",
   "metadata": {},
   "source": [
    "Sort the dictionary so the speakers with the most quotes appear first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9df134db-4833-4b14-b179-9e8070e3bd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_combined_signi_dict = {k: v for k, v in sorted(combined_signi_dict.items(), key=lambda item: item[1], reverse=True)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5cf729-0697-4a88-94b3-8faf7cd737b1",
   "metadata": {},
   "source": [
    "And finally save the resulting dictionary into a file, this file can later be reused for multiple analyses, whenever we need to choose a representation of a group of people using the number of quotes to pick the most quoted individuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeabe2a1-4436-45b9-be62-df9fd8069e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_json_to_file('signi-quote-count-combined', sorted_combined_signi_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12018aae-3a5a-48a0-9e74-12f0a21763a9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Get the wikidata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f674fda1-e30d-4499-8588-6d1e1092fbe3",
   "metadata": {},
   "source": [
    "We used the https://query.wikidata.org/ website to get the relevant wikidata. The SPARQL query is in the following cell.\n",
    "<br>\n",
    "We can do this (and did do afterwards) using the provided wikidata parquet file as well."
   ]
  },
  {
   "cell_type": "raw",
   "id": "507dbcdb-b7e6-4ba6-86d9-f663272730e8",
   "metadata": {},
   "source": [
    "SELECT DISTINCT ?item ?itemLabel \n",
    "          ?genderLabel ?citizenshipLabel ?languageLabel ?religionLabel ?ethnicLabel ?degreeLabel\n",
    "          ?dateOfBirth ?placeOfBirthLabel \n",
    "#           ?nativeNameLabel ?birthNameLabel ?givenNameLabel ?familyNameLabel ?pseudonymLabel \n",
    "#           ?fatherLabel ?motherLabel ?siblingLabel ?spouseLabel ?childLabel ?numOfChild \n",
    "#           ?occupationLabel ?positionLabel ?ideologyLabel ?educatedAtLabel\n",
    "          ?memberOfParty ?memberOfPartyLabel \n",
    "WHERE {\n",
    "  SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE]\". }\n",
    "  {\n",
    "    ?item p:P106 ?statement0.\n",
    "    ?statement0 (ps:P106) wd:Q82955.\n",
    "    {\n",
    "      ?item p:P102 ?statement1.\n",
    "      ?statement1 (ps:P102) wd:Q29552.\n",
    "    }\n",
    "    UNION\n",
    "    {\n",
    "      ?item p:P102 ?statement2.\n",
    "      ?statement2 (ps:P102) wd:Q29468.\n",
    "    }\n",
    "    MINUS {\n",
    "      ?item p:P570 ?statement_3.\n",
    "      ?statement_3 psv:P570 ?statementValue_3.\n",
    "      ?statementValue_3 wikibase:timePrecision ?precision_3.\n",
    "      FILTER(?precision_3 >= 11 )\n",
    "      ?statementValue_3 wikibase:timeValue ?P570_3.\n",
    "      FILTER(?P570_3 < \"+2015-01-01T00:00:00Z\"^^xsd:dateTime)\n",
    "    }\n",
    "    OPTIONAL { ?item wdt:P21 ?gender. }\n",
    "    OPTIONAL { ?item wdt:P27 ?citizenship. }\n",
    "    OPTIONAL { ?item wdt:P103 ?language. }\n",
    "    OPTIONAL { ?item wdt:P140 ?religion. }\n",
    "    OPTIONAL { ?item wdt:P172 ?ethnic. }\n",
    "    OPTIONAL { ?item wdt:P512 ?degree. }\n",
    "    \n",
    "    OPTIONAL { ?item wdt:P569 ?dateOfBirth. }\n",
    "    OPTIONAL { ?item wdt:P19 ?placeOfBirth. }\n",
    "    \n",
    "#     OPTIONAL { ?item wdt:P1559 ?nativeName. }\n",
    "#     OPTIONAL { ?item wdt:P1477 ?birthName. }\n",
    "#     OPTIONAL { ?item wdt:P735 ?givenName. }\n",
    "#     OPTIONAL { ?item wdt:P734 ?familyName. }\n",
    "#     OPTIONAL { ?item wdt:P742 ?pseudonym. }\n",
    "    \n",
    "#     OPTIONAL { ?item wdt:P22 ?father. }\n",
    "#     OPTIONAL { ?item wdt:P25 ?mother. }\n",
    "#     OPTIONAL { ?item wdt:P3373 ?sibling. }\n",
    "#     OPTIONAL { ?item wdt:P26 ?spouse. }\n",
    "#     OPTIONAL { ?item wdt:P40 ?child. }\n",
    "#     OPTIONAL { ?item wdt:P1971 ?numOfChild. }\n",
    "    \n",
    "#     OPTIONAL { ?item wdt:P106 ?occupation. }\n",
    "#     OPTIONAL { ?item wdt:P39 ?position. }\n",
    "#     OPTIONAL { ?item wdt:P1142 ?ideology. }\n",
    "#     OPTIONAL { ?item wdt:P69 ?educatedAt. }\n",
    "    \n",
    "    OPTIONAL { ?item wdt:P102 ?memberOfParty. }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d7475f-fb4d-4f46-9b9e-704f22e7631e",
   "metadata": {},
   "source": [
    "Merge duplicate objects representing a single speaker but with differing fields.\n",
    "<br>\n",
    "Example: Arnold Schwarzenegger has both Austrian and American nationalities, and would appear twice, once with Austrian, and once with American nationality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4fbda022-cad4-42ec-9715-18a6a7e346ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../quotebank/american_politicians_fixed.json\", \"r\") as f:\n",
    "    wiki_poli = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c56b066b-505e-48a5-811f-de907c067193",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_poli_merged = dict()\n",
    "\n",
    "index = 0\n",
    "for row in wiki_poli:\n",
    "    # Extract the QID from the link (ex. http://www.wikidata.org/entity/Q203286 -> Q203286)\n",
    "    qid_start = row['item'].rindex('/') + 1\n",
    "    key = row['item'][qid_start:]\n",
    "    # Replace the link with the QID\n",
    "    row['item'] = key\n",
    "    \n",
    "    if key in wiki_poli_merged:\n",
    "        merged_entry = wiki_poli_merged[key]\n",
    "        columns = ['itemLabel', 'genderLabel', 'citizenshipLabel', 'religionLabel', 'ethnicLabel', 'degreeLabel', 'dateOfBirth', 'placeOfBirthLabel', 'memberOfParty', 'memberOfPartyLabel', 'languageLabel']\n",
    "        \"\"\"\n",
    "        Merge the values for every column:\n",
    "            - if the values are the same - do nothing\n",
    "            - if the values are different - create a list and add them both\n",
    "        \"\"\"\n",
    "        for col in columns:\n",
    "            if row.get(col, None) is None:\n",
    "                continue\n",
    "                \n",
    "            updated_entry = merged_entry.get(col, None)\n",
    "            \n",
    "            if updated_entry is None:\n",
    "                updated_entry = row[col]\n",
    "            elif isinstance(updated_entry, list):\n",
    "                if row[col] not in updated_entry:\n",
    "                    updated_entry.append(row[col])\n",
    "            elif row[col] != updated_entry:\n",
    "                updated_entry = [updated_entry, row[col]]\n",
    "                \n",
    "            merged_entry[col] = updated_entry\n",
    "    else:\n",
    "        wiki_poli_merged[key] = row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b884918a-8b0f-46bf-a9fc-6c28a0f48645",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'american_politicians_final_1636570708023.json'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "write_json_to_file('american_politicians_final', wiki_poli_merged)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3eb4e4a-00d7-461d-a6ee-0fcc4d85d707",
   "metadata": {},
   "source": [
    "### Get the 100 most quoted party members"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c17cd2e-0646-4175-971f-68f6f3e24d90",
   "metadata": {},
   "source": [
    "Using the results of the previous two steps - the number of quotes for each speaker, and the list of US politicians, we can compile a list of 100 most quoted members of the two major US political parties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2e228fe3-ebfd-4803-b998-54c2be188e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_list = []\n",
    "rep_list = []\n",
    "\n",
    "CAP_TARGET = 100\n",
    "DEM_PARTY = \"http://www.wikidata.org/entity/Q29552\"\n",
    "REP_PARTY = \"http://www.wikidata.org/entity/Q29468\"\n",
    "\n",
    "for v in sorted_combined_signi_dict:\n",
    "    row = wiki_poli_merged.get(v, None)\n",
    "    \n",
    "    # Could not find person in the politician dictionary\n",
    "    if row is None:\n",
    "        continue\n",
    "    \n",
    "    memberOfParty = row.get('memberOfParty', None)\n",
    "    if memberOfParty is None:\n",
    "        continue\n",
    "    \n",
    "    # Cast to one element list if not already a list\n",
    "    if isinstance(memberOfParty, list) == False:\n",
    "        memberOfParty = [memberOfParty]\n",
    "    \n",
    "    # Check membership\n",
    "    if DEM_PARTY in memberOfParty:\n",
    "        if REP_PARTY in memberOfParty:\n",
    "            # member of both parties, just skip\n",
    "            continue\n",
    "\n",
    "        # Check if the list is already at full capacity\n",
    "        if len(dem_list) < CAP_TARGET:\n",
    "            dem_list.append(row)\n",
    "    elif REP_PARTY in memberOfParty:\n",
    "        # Check if the list is already at full capacity\n",
    "        if len(rep_list) < CAP_TARGET:\n",
    "            rep_list.append(row)\n",
    "    \n",
    "    # Check if both lists are at full capacity\n",
    "    if len(dem_list) == CAP_TARGET and len(rep_list) == CAP_TARGET:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3f8832-5908-449a-aa39-f54b5e35e629",
   "metadata": {},
   "source": [
    "### Get the politician quotes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144fa20f-4433-498f-bd8b-bc2d9580293a",
   "metadata": {},
   "source": [
    "For the politicians in the previously compiled lists, we now fetch the quotes from the quote files. We use the methods defined at the top of this notebook, which were written in a reusable way."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9618f4-4d12-4d48-921e-403c88fee32b",
   "metadata": {},
   "source": [
    "Define the initialization and visit methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a58f1a2d-722c-49a9-9bb5-a71e01c1049e",
   "metadata": {},
   "outputs": [],
   "source": [
    "poli_quotes = {}\n",
    "poli_people = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "1b22d807-3800-4807-b6c3-01454504267e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def poli_initialize(out_file):\n",
    "    global poli_quotes\n",
    "    global poli_people\n",
    "    global dem_list\n",
    "    global rep_list\n",
    "    \n",
    "    poli_quotes = {}\n",
    "    poli_people = set()\n",
    "    \n",
    "    for v in dem_list:\n",
    "        poli_people.add(v['item'])\n",
    "    for v in rep_list:\n",
    "        poli_people.add(v['item'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a3cb0a9b-3e5a-4fd4-bccc-a005c4dde7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Remember the quote, only if it belongs to one of the politicians in the set poli_people, and if the probability is over 80%.\n",
    "\"\"\"\n",
    "def save_politician_quotes(out_file, row):\n",
    "    global poli_quotes\n",
    "    global poli_people\n",
    "    \n",
    "    probas = row['probas']\n",
    "    qids = row['qids']\n",
    "    \n",
    "    # Check if the probability field exists\n",
    "    if (len(probas) == 0 or len(qids) == 0):\n",
    "        return\n",
    "    \n",
    "    if (probas[0][0] == 'None'):\n",
    "        return\n",
    "    \n",
    "    # Check if the probability is over 80%\n",
    "    p = float(probas[0][1])\n",
    "    if (p < 0.8):\n",
    "        return\n",
    "    \n",
    "    # Check if the speaker is one of the 100 party members\n",
    "    qid = qids[0]\n",
    "    if qid not in poli_people:\n",
    "        return\n",
    "    \n",
    "    # Remember only the quote and the probability\n",
    "    data = {}\n",
    "    data['quotation'] = row['quotation']\n",
    "    data['proba'] = row['probas'][0][1]\n",
    "    \n",
    "    # Append the quote\n",
    "    arr = poli_quotes.get(qid, [])\n",
    "    arr.append(data)\n",
    "    poli_quotes[qid] = arr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee80d1f-1b4d-4b12-b480-b98be3534c4b",
   "metadata": {},
   "source": [
    "Create files for every year, each file contains a dictionary where the key is the QID of the speaker, and the value is the list of significant quotes attributed to the speaker.\n",
    "<br><br>\n",
    "<font color='red'>WARNING: LONG EXECUTION!</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d43542e-879b-4e5b-a583-1b78db048da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "years = [2015, 2016, 2017, 2018, 2019, 2020]\n",
    "# years = [2020]\n",
    "for year in years:\n",
    "    path_to_input = PATTERN_INPUT.format(year)\n",
    "    \n",
    "    run_through_quotes(\n",
    "        poli_initialize, save_politician_quotes, year, \"poli_quotes\", path_to_input, name='politician-quotes', chunk_size=1_048_576)\n",
    "    print('')\n",
    "    print(f'Finished compiling quotes for the year {year}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7ffd50-cad0-4be1-81f5-dae693471721",
   "metadata": {},
   "source": [
    "### Combine the quotes and the wikidata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30180476-46c2-400d-a424-e9f7a0757a56",
   "metadata": {},
   "source": [
    "Now we combine the politician quotes with their wikidata information. We use the 6 files of politician quotes created in the previous step, as well as the list of the party members. The result is a file which contains 200 entries, where each entry represents one politician, and contains their wikidata info as well as a list of quotes. The list of quotes can be quite long for some of the politicians."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "ba1bacad-9fb3-407f-b6ba-1a4422135c2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'politician-quotes-combined_1636575214462.json'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poli_quote_files = [\n",
    "    \"../quotebank/politician-quotes-2015_1636331534906.json\",\n",
    "    \"../quotebank/politician-quotes-2016_1636332058163.json\",\n",
    "    \"../quotebank/politician-quotes-2017_1636333168732.json\",\n",
    "    \"../quotebank/politician-quotes-2018_1636334221167.json\",\n",
    "    \"../quotebank/politician-quotes-2019_1636335010497.json\",\n",
    "    \"../quotebank/politician-quotes-2020_1636330658142.json\"\n",
    "]\n",
    "\n",
    "poli_quotes_combined = {}\n",
    "\n",
    "both_parties = dem_list + rep_list\n",
    "for v in both_parties:\n",
    "    copy = dict(v)\n",
    "    copy['quotations'] = []\n",
    "    \n",
    "    poli_quotes_combined[v['item']] = copy\n",
    "\n",
    "for poli_quote_file_name in poli_quote_files:\n",
    "    with open(poli_quote_file_name, 'r') as f:\n",
    "        quotes = json.load(f)\n",
    "        \n",
    "        for k in quotes.keys():\n",
    "            poli_quotes_combined[k]['quotations'] += quotes[k]\n",
    "\n",
    "write_json_to_file('politician-quotes-combined', poli_quotes_combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3147ad-1297-4bf9-a60b-fde7fe4bb168",
   "metadata": {},
   "source": [
    "### Filter the quotes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993e2f8a-7e2d-45f9-b283-db99cceb3db2",
   "metadata": {},
   "source": [
    "Some of the quotes in the database do not represent actual quotes, but instead contain junk like html tags, source code, or text from the webpage where the source article was published.\n",
    "<br>\n",
    "We filter these quotes out so our dataset is not polluted by junk data. We have found a few filters which detect most of the junk data, while maintaining a low false positive rate:\n",
    "<ul>\n",
    "    <li>quotes which contains very long 'words' - more than 50 characters</li>\n",
    "    <li>quotes which contain URLs - these usually contain other junk characters</li>\n",
    "    <li>quotes which contains JSON-like key-value pairs</li>\n",
    "    <li>quotes which contain a lot of special characters (more than 10% of total characters)</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "e49ed80f-0a0d-4ead-9beb-8b5447e527fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../quotebank/politician-quotes-combined_1636336204264.json', 'r') as f:\n",
    "    poli_quotes_filtered = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "ca6885b3-4f20-46f4-96c5-0eb4c6ef923e",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_quotes = []\n",
    "\n",
    "weird_pattern = '[_@#+&;:\\(\\)\\{\\}\\[\\]\\\\/`]'\n",
    "json_pattern = '\\{.*[a-zA-Z]+:\\s[\\'\"`][a-zA-Z0-9]+[\\'\"`].*\\}'\n",
    "url_pattern = 'https?'\n",
    "\n",
    "for k in poli_quotes_filtered.keys():\n",
    "    elem = poli_quotes_filtered[k]\n",
    "    \n",
    "    new_arr = []\n",
    "    for entry in elem['quotations']:\n",
    "        text = entry['quotation']\n",
    "        \n",
    "        longest = max(entry['quotation'].split(), key=len)\n",
    "        if (len(longest) > 50):\n",
    "            filtered_quotes.append(entry)\n",
    "            continue\n",
    "        \n",
    "        if re.search(url_pattern, text) is not None:\n",
    "            filtered_quotes.append(entry)\n",
    "            continue\n",
    "        \n",
    "        if re.search(json_pattern, text) is not None:\n",
    "            filtered_quotes.append(entry)\n",
    "            continue\n",
    "            \n",
    "        weird_num = len(re.findall(weird_pattern, text))\n",
    "        total = len(text)\n",
    "        weird_percent = weird_num / total\n",
    "        if (weird_percent > 0.1):\n",
    "            filtered_quotes.append(entry)\n",
    "            continue\n",
    "            \n",
    "        new_arr.append(entry)\n",
    "    elem['quotations'] = new_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "8a3ed2bb-4806-452b-931d-df96cc058994",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/politician-quotes-combined-and-filtered_1636577222699.json'"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "write_json_to_file('data/politician-quotes-combined-and-filtered', poli_quotes_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29ea8c3-b33e-4988-a18c-c26c871cd41b",
   "metadata": {},
   "source": [
    "### Concatenate the quotes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c179925a-2e23-4a98-91e0-1c5845426efc",
   "metadata": {},
   "source": [
    "Finally, we concatenate the quotes into a single fixed-length string. We do this because of the limitation of the CSV file format, which can contains at most ~32000 characters in a single field. This means that most of the quotes will not be used.\n",
    "<br>\n",
    "Alternatively, we could use multiple fields for the same speaker, but we think the amount of characters that can fit in a single cell is enough for a decent analysis.\n",
    "<br>\n",
    "We sort the quotes by length and use the longest ones first. We do this because the longer quotes are a better representation of a person's speach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "4156eae2-9288-477d-bdb9-258de315a476",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/politician-quotes-combined-and-filtered_1636577222699.json', 'r') as f:\n",
    "    poli_quotes_concat = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "a02ee34d-512c-4ce3-8fbf-1a52ad915c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUOTE_LENGTH = 5000\n",
    "\n",
    "for k in poli_quotes_concat.keys():\n",
    "    elem = poli_quotes_concat[k]\n",
    "    \n",
    "    # Sort the quotes by length\n",
    "    elem['quotations'].sort(key = lambda x: len(x['quotation']), reverse = True)\n",
    "    \n",
    "    concat = ''\n",
    "    for quote in elem['quotations']:\n",
    "        # Concatenate the quotes\n",
    "        concat += ' ' + quote['quotation']\n",
    "        \n",
    "        # Trim if we are over QUOTE_LENGTH\n",
    "        if (len(concat) >= QUOTE_LENGTH):\n",
    "            concat = concat[0:QUOTE_LENGTH]\n",
    "            break\n",
    "    \n",
    "    elem['quotations'] = concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d600af-93ec-400f-bbf1-b9442ca1bd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_dict_to_file('data/politician-quotes-concatenated', poli_quotes_concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbe3383-769e-4af8-a31b-f04ad3454796",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "52ead77e",
   "metadata": {},
   "source": [
    "## Initial Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b58705c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bz2\n",
    "import json\n",
    "import pandas as pd\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cdaee992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the concatenated quotes for top 100 politician\n",
    "with open('./data/politician-quotes-concatenated_1636411537251.json', 'r') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c597066",
   "metadata": {},
   "source": [
    "After getting the data, we extract the quote ID and the concatenated quote of each politician, and write them to `input_data1.csv` for the LIWC personality analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06aaf991",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('input_data1.csv', 'w', encoding='UTF8', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"qid\", \"quote\"])\n",
    "    for qid, all_value in data.items():\n",
    "        quote = all_value[\"quotations\"]\n",
    "        writer.writerow([qid, quote])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6959ee03",
   "metadata": {},
   "source": [
    "### LIWC Analysis\n",
    "After parsing the `input_data1.csv` using the liwc software (Academic Version), for each concatenated quote, it gains a list of features in terms of LIWC categories, such as pronoun, articles. We save the data as `output_data1.csv` and then load it to our notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5601021b",
   "metadata": {},
   "outputs": [],
   "source": [
    "liwc = pd.read_csv('output_data1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f2370b7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source (A)</th>\n",
       "      <th>Source (B)</th>\n",
       "      <th>WC</th>\n",
       "      <th>WPS</th>\n",
       "      <th>Sixltr</th>\n",
       "      <th>Dic</th>\n",
       "      <th>Pronoun</th>\n",
       "      <th>I</th>\n",
       "      <th>We</th>\n",
       "      <th>Self</th>\n",
       "      <th>...</th>\n",
       "      <th>Comma</th>\n",
       "      <th>Colon</th>\n",
       "      <th>SemiC</th>\n",
       "      <th>QMark</th>\n",
       "      <th>Exclam</th>\n",
       "      <th>Dash</th>\n",
       "      <th>Quote</th>\n",
       "      <th>Apostro</th>\n",
       "      <th>Parenth</th>\n",
       "      <th>OtherP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Q1077594</td>\n",
       "      <td>The NCAA and collegiate sports more broadly n...</td>\n",
       "      <td>825</td>\n",
       "      <td>14.73</td>\n",
       "      <td>28.36</td>\n",
       "      <td>68.36</td>\n",
       "      <td>7.64</td>\n",
       "      <td>1.45</td>\n",
       "      <td>1.7</td>\n",
       "      <td>3.15</td>\n",
       "      <td>...</td>\n",
       "      <td>2.79</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.45</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 86 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Source (A)                                         Source (B)   WC    WPS  \\\n",
       "12   Q1077594   The NCAA and collegiate sports more broadly n...  825  14.73   \n",
       "\n",
       "    Sixltr    Dic  Pronoun     I   We  Self  ...  Comma  Colon  SemiC  QMark  \\\n",
       "12   28.36  68.36     7.64  1.45  1.7  3.15  ...   2.79   0.12    0.0    0.0   \n",
       "\n",
       "    Exclam  Dash  Quote  Apostro  Parenth  OtherP  \n",
       "12     0.0  2.06    0.0     1.45      0.0    0.12  \n",
       "\n",
       "[1 rows x 86 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualise a random sample\n",
    "liwc.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0512840",
   "metadata": {},
   "source": [
    "### Personality Analysis Based on LIWC Results\n",
    "According to the research by Tal Yarkoni from University of Colorado at Boulder, significant correlations between LIWC categories and the big five personalities are identified based on a large scale analysis (2010). Hence, we create the `predict_personality()` function which allows us to select the features based on the recorded significant level from the research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e90f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_personality(liwc_data: pd.DataFrame, sig_level: int = 1) -> pd.DataFrame:\n",
    "    \"\"\"Predicts personality based on the LIWC metrics\n",
    "\n",
    "    Args:\n",
    "        liwc_data (pd.DataFrame): LIWC metrics\n",
    "        sig_level (int, optional): Significance level. Defaults to 3 (i.e. greater than 0.001)\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Personality scores\n",
    "    \"\"\"\n",
    "    liwc_ocean_data = pd.read_csv('data/LIWC_OCEAN.csv', index_col=0)\n",
    "    liwc_ocean_sig_data = pd.read_csv('data/LIWC_OCEAN_Significance.csv', index_col=0)\n",
    "    liwc_data = liwc_data[list(LIWC_OCEAN_MAP.keys())].rename(columns=LIWC_OCEAN_MAP)\n",
    "    liwc_data = liwc_data.div(liwc_data.sum(axis=1), axis=0)\n",
    "    assert (liwc_ocean_data.index == liwc_data.columns).all()\n",
    "    liwc_ocean_data_with_sig = liwc_ocean_data * (liwc_ocean_sig_data >= sig_level).astype(int)\n",
    "    return liwc_data.dot(liwc_ocean_data_with_sig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bde3c863",
   "metadata": {},
   "outputs": [],
   "source": [
    "import helpers\n",
    "# call the predict_personality function from helpers file which include the name cleaning map for liwc.\n",
    "bigfive = helpers.predict_personality(liwc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5b51fb47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>neuroticism</th>\n",
       "      <th>anxiety</th>\n",
       "      <th>hostility</th>\n",
       "      <th>depression</th>\n",
       "      <th>self_consciousness</th>\n",
       "      <th>immoderation</th>\n",
       "      <th>vulnerability</th>\n",
       "      <th>extraversion</th>\n",
       "      <th>friendliness</th>\n",
       "      <th>gregariousness</th>\n",
       "      <th>...</th>\n",
       "      <th>cooperation</th>\n",
       "      <th>modesty</th>\n",
       "      <th>sympathy</th>\n",
       "      <th>conscientiousness</th>\n",
       "      <th>self_efficacy</th>\n",
       "      <th>orderliness</th>\n",
       "      <th>dutifulness</th>\n",
       "      <th>achievement_striving</th>\n",
       "      <th>self_discipline</th>\n",
       "      <th>cautiousness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>0.920334</td>\n",
       "      <td>-0.038939</td>\n",
       "      <td>2.156352</td>\n",
       "      <td>-0.049568</td>\n",
       "      <td>0.14622</td>\n",
       "      <td>-0.185656</td>\n",
       "      <td>-0.397437</td>\n",
       "      <td>2.485448</td>\n",
       "      <td>3.972683</td>\n",
       "      <td>3.314592</td>\n",
       "      <td>...</td>\n",
       "      <td>3.109864</td>\n",
       "      <td>0.954405</td>\n",
       "      <td>2.603854</td>\n",
       "      <td>-1.471541</td>\n",
       "      <td>2.190921</td>\n",
       "      <td>0.337737</td>\n",
       "      <td>0.779478</td>\n",
       "      <td>-1.747988</td>\n",
       "      <td>-1.446806</td>\n",
       "      <td>0.423959</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     neuroticism   anxiety  hostility  depression  self_consciousness  \\\n",
       "117     0.920334 -0.038939   2.156352   -0.049568             0.14622   \n",
       "\n",
       "     immoderation  vulnerability  extraversion  friendliness  gregariousness  \\\n",
       "117     -0.185656      -0.397437      2.485448      3.972683        3.314592   \n",
       "\n",
       "     ...  cooperation   modesty  sympathy  conscientiousness  self_efficacy  \\\n",
       "117  ...     3.109864  0.954405  2.603854          -1.471541       2.190921   \n",
       "\n",
       "     orderliness  dutifulness  achievement_striving  self_discipline  \\\n",
       "117     0.337737     0.779478             -1.747988        -1.446806   \n",
       "\n",
       "     cautiousness  \n",
       "117      0.423959  \n",
       "\n",
       "[1 rows x 35 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigfive.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fddb9e42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source (A)</th>\n",
       "      <th>Source (B)</th>\n",
       "      <th>WC</th>\n",
       "      <th>WPS</th>\n",
       "      <th>Sixltr</th>\n",
       "      <th>Dic</th>\n",
       "      <th>Pronoun</th>\n",
       "      <th>I</th>\n",
       "      <th>We</th>\n",
       "      <th>Self</th>\n",
       "      <th>...</th>\n",
       "      <th>cooperation</th>\n",
       "      <th>modesty</th>\n",
       "      <th>sympathy</th>\n",
       "      <th>conscientiousness</th>\n",
       "      <th>self_efficacy</th>\n",
       "      <th>orderliness</th>\n",
       "      <th>dutifulness</th>\n",
       "      <th>achievement_striving</th>\n",
       "      <th>self_discipline</th>\n",
       "      <th>cautiousness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>Q934898</td>\n",
       "      <td>And I see my father. My father was just wipin...</td>\n",
       "      <td>969</td>\n",
       "      <td>17.3</td>\n",
       "      <td>10.42</td>\n",
       "      <td>85.86</td>\n",
       "      <td>15.58</td>\n",
       "      <td>4.02</td>\n",
       "      <td>3.82</td>\n",
       "      <td>7.84</td>\n",
       "      <td>...</td>\n",
       "      <td>1.246182</td>\n",
       "      <td>1.163683</td>\n",
       "      <td>1.270105</td>\n",
       "      <td>-1.796902</td>\n",
       "      <td>0.643346</td>\n",
       "      <td>0.97126</td>\n",
       "      <td>0.736629</td>\n",
       "      <td>-1.169705</td>\n",
       "      <td>-1.303115</td>\n",
       "      <td>-0.2487</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 121 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Source (A)                                         Source (B)   WC   WPS  \\\n",
       "51    Q934898   And I see my father. My father was just wipin...  969  17.3   \n",
       "\n",
       "    Sixltr    Dic  Pronoun     I    We  Self  ...  cooperation   modesty  \\\n",
       "51   10.42  85.86    15.58  4.02  3.82  7.84  ...     1.246182  1.163683   \n",
       "\n",
       "    sympathy  conscientiousness  self_efficacy  orderliness  dutifulness  \\\n",
       "51  1.270105          -1.796902       0.643346      0.97126     0.736629   \n",
       "\n",
       "    achievement_striving  self_discipline  cautiousness  \n",
       "51             -1.169705        -1.303115       -0.2487  \n",
       "\n",
       "[1 rows x 121 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# concat the liwc output to the bigfive result\n",
    "df1 = pd.concat([liwc, bigfive], axis=1)\n",
    "df1.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fc5d2315",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source (A)</th>\n",
       "      <th>Source (B)</th>\n",
       "      <th>WC</th>\n",
       "      <th>WPS</th>\n",
       "      <th>Sixltr</th>\n",
       "      <th>Dic</th>\n",
       "      <th>Pronoun</th>\n",
       "      <th>I</th>\n",
       "      <th>We</th>\n",
       "      <th>Self</th>\n",
       "      <th>...</th>\n",
       "      <th>citizenshipLabel</th>\n",
       "      <th>languageLabel</th>\n",
       "      <th>religionLabel</th>\n",
       "      <th>ethnicLabel</th>\n",
       "      <th>degreeLabel</th>\n",
       "      <th>dateOfBirth</th>\n",
       "      <th>placeOfBirthLabel</th>\n",
       "      <th>memberOfParty</th>\n",
       "      <th>memberOfPartyLabel</th>\n",
       "      <th>party</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>Q16215328</td>\n",
       "      <td>A lot of good takeaways from this weekend. A ...</td>\n",
       "      <td>873</td>\n",
       "      <td>19.84</td>\n",
       "      <td>20.73</td>\n",
       "      <td>70.9</td>\n",
       "      <td>7.22</td>\n",
       "      <td>0.34</td>\n",
       "      <td>2.63</td>\n",
       "      <td>2.98</td>\n",
       "      <td>...</td>\n",
       "      <td>United States of America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1975-01-01T00:00:00Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://www.wikidata.org/entity/Q29552</td>\n",
       "      <td>Democratic Party</td>\n",
       "      <td>dem</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 134 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Source (A)                                         Source (B)   WC    WPS  \\\n",
       "78  Q16215328   A lot of good takeaways from this weekend. A ...  873  19.84   \n",
       "\n",
       "    Sixltr   Dic  Pronoun     I    We  Self  ...          citizenshipLabel  \\\n",
       "78   20.73  70.9     7.22  0.34  2.63  2.98  ...  United States of America   \n",
       "\n",
       "    languageLabel  religionLabel  ethnicLabel  degreeLabel  \\\n",
       "78            NaN            NaN          NaN          NaN   \n",
       "\n",
       "             dateOfBirth  placeOfBirthLabel  \\\n",
       "78  1975-01-01T00:00:00Z                NaN   \n",
       "\n",
       "                            memberOfParty  memberOfPartyLabel  party  \n",
       "78  http://www.wikidata.org/entity/Q29552    Democratic Party    dem  \n",
       "\n",
       "[1 rows x 134 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the top 100 politician data, queried from wikidata, concat with current dataset.\n",
    "with open('./data/top100_politicians_by_party.json', 'r') as f:\n",
    "    data_top100 = json.load(f)\n",
    "\n",
    "dem_df = pd.DataFrame(data_top100[\"dem\"])\n",
    "rep_df = pd.DataFrame(data_top100[\"rep\"])\n",
    "dem_df['party'] = \"dem\"\n",
    "rep_df['party'] = \"rep\"\n",
    "politician_wiki = pd.concat([dem_df, rep_df])\n",
    "df2 = df1.merge(politician_wiki, left_on='Source (A)', right_on='item', how = \"left\")\n",
    "df2.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0fa4ee7",
   "metadata": {},
   "source": [
    "## Basic Analysis\n",
    "\n",
    "For this part, we use the programming language R to get better visualisation and embed the output in html by knitting the Rmarkdown document."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2db5a5e",
   "metadata": {},
   "source": [
    "### Comparing the Personality of Politicians from Democratic and Republic Parties\n",
    "\n",
    "We compare each characteristic for democratic and republic politicians using Wilcoxon rank sum test, which tests whether top politicians from different parties have the equal medians for each attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "27e2d066",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"700\"\n",
       "            height=\"600\"\n",
       "            src=\"./section-1.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f80b2d786d0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame(src='./section-1.html', width=700, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfc4e0a",
   "metadata": {},
   "source": [
    "The current result shows that the only significant differences are artistic_interests and emotionality, where democratic politicians have higher average artistic_interests and emotionality compared to republic politicians."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367934de",
   "metadata": {},
   "source": [
    "### Comparing Main Politicians in US Based on Their Personality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b997534",
   "metadata": {},
   "source": [
    "We use interactive heatmap in r to produce the following map. By clicking on the specific cell, you can see the value of each attribute for the person and compare it to other politicians. When a cell is blue, it means a positive value with regards to that attribute. When it is red, it gives a negative value for the respective characteristic.\n",
    "\n",
    "Moreover, by selecting several cells, you are able to zoom in to see difference in details. To go back to the original view, please double click the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9a5a45ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"700\"\n",
       "            height=\"600\"\n",
       "            src=\"./section-2.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f80afbd3990>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IFrame(src='./section-2.html', width=700, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8022b36",
   "metadata": {},
   "source": [
    "On the plot above, similar people will be scattered together. For example,\n",
    "- Barack Obama and George W. Bush have similar personalities based on their quotations. \n",
    "- Donald Trump and Lindsey Graham are quite close on most scales of personality. \n",
    "- Obama and Trump seem to have opposite personality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba174c66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
