{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4de39a13-5825-42d1-a63e-262105ce4dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bz2\n",
    "import json\n",
    "import re\n",
    "import random\n",
    "import sys\n",
    "import os\n",
    "import bz2\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "import seaborn as sns\n",
    "from helpers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90353947-3416-4db8-a71e-5a5467762788",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_json_to_file(name, obj):\n",
    "    # Use current timestamp to make the name of the file unique\n",
    "    millis = round(time.time() * 1000)\n",
    "    name = f'{name}_{millis}.json'\n",
    "    with open(name, 'wb') as f:\n",
    "        output = json.dumps(obj)\n",
    "        f.write(output.encode('utf-8'))\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6657f1fa-6a32-4fa3-9e76-ff5fa0ad734e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/local_only/signi-quote-count-combined_1636658426963.json\", \"r\") as f:\n",
    "    sorted_combined_signi_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b4e29b06-d8b5-4cf7-ab50-f71204c29640",
   "metadata": {},
   "outputs": [],
   "source": [
    "signi_list = list(sorted_combined_signi_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "59cd6f73-649c-4b22-9ab0-c6a86dc2df2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOUND: #1026 2689\n"
     ]
    }
   ],
   "source": [
    "test_list = list(sorted_combined_signi_dict.items())\n",
    "\n",
    "found_anthony = False\n",
    "RANGE = 2000\n",
    "\n",
    "for i in range(RANGE):\n",
    "#     print(test_list[i])\n",
    "    qid = test_list[i][0]\n",
    "    \n",
    "    if qid == 'Q426582':\n",
    "        print(f'FOUND: #{i} {test_list[i][1]}')\n",
    "        found_anthony = True\n",
    "\n",
    "if found_anthony == False:\n",
    "    print(f'not found in the top {RANGE}')\n",
    "    ind = RANGE - 1\n",
    "    print(f'#{RANGE} list[{test_list[ind][0]}] = {test_list[ind][1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "16e7f227-3241-4fa3-a26d-6dee1612201c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list[    0] = 201293\n",
      "list[  500] =  4147\n",
      "list[ 1000] =  2720\n",
      "list[ 1500] =  2114\n"
     ]
    }
   ],
   "source": [
    "x = 0\n",
    "\n",
    "for i in range(4):\n",
    "    print(f'list[{x:5}] = {signi_list[x][1]:5}')\n",
    "    x += 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0efcf158-3f13-48e4-8842-ec807cbaf7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "top1000 = signi_list[0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "92a094e2-9033-4ae7-a820-d1138c4452dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(top1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6156b5fa-1cc0-4efc-b2ca-340b856d1202",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Q22686', 'Q1058', 'Q76', 'Q450675', 'Q83106']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top1000[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c3e25c1a-566b-4333-a5a9-19a494b16608",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATTERN_INPUT = \"../quotebank/quotes-{}.json.bz2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ac53b4d9-58ab-4539-be5d-01b55d509997",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "CHUNK_SIZE = 1_048_576\n",
    "\n",
    "def process_compressed_json_file(input_file_name: str, output_name: str, year: int, process_json_object: Callable) -> str:\n",
    "    \"\"\"\n",
    "    Read from a compressed file chunk by chunk. Decompress every chunk and try to decode it and parse it into an array of JSON objects.\n",
    "    For each JSON object extracted this way, run the process_json_object function.\n",
    "    In the end, a JSON object representing the result of this process is written into a file.\n",
    "\n",
    "    Args:\n",
    "        input_file_name (str): Name of the compressed json file which is the subject of processing.\n",
    "        output_name (str): First part of the output file name. Used in creation of the full output file name: the year parameter and a timestamp are appended, as well as the .json extension.\n",
    "        year (int): Represents the year for which the data in the input file is gathered, is appended to the output_name to generate the full output file name.\n",
    "        process_json_object (Callable): Function that processes the individual JSON objects extracted from the compressed file. The signature should be as follows:\n",
    "            Args:\n",
    "                json_obj: JSON object which is to be processed.\n",
    "                out_json: The output object in which the result of the processing is stored\n",
    "\n",
    "    Returns:\n",
    "        (str) Full name of the output JSON file.\n",
    "    \"\"\"\n",
    "    # Decompression variables\n",
    "    decompressor = bz2.BZ2Decompressor()\n",
    "    \n",
    "    # Decoding variables\n",
    "    decoding_buffer = bytearray([])\n",
    "    decoding_error_counter = 0\n",
    "    \n",
    "    # Parsing variables\n",
    "    parsing_buffer = ''\n",
    "    parsing_error_counter = 0\n",
    "    \n",
    "    # Progress variables - used to provide feedback to the dev\n",
    "    input_size = os.path.getsize(input_file_name)\n",
    "    start_time = time.time()\n",
    "    total_in = 0\n",
    "    total_out = 0\n",
    "    previous_value = -1\n",
    "    \n",
    "    # Result of processing\n",
    "    out_json = dict()\n",
    "    \n",
    "    # Iterate through the file\n",
    "    with open(input_file_name, 'rb') as input_file:\n",
    "        for chunk in iter(lambda: input_file.read(CHUNK_SIZE), b''):\n",
    "            # Feed chunk to decompressor\n",
    "            decompressed_chunk = decompressor.decompress(chunk)\n",
    "            dec_chunk_length = len(decompressed_chunk)\n",
    "            \n",
    "            # Check the length of the decompressed data - 0 is common -- waiting for a bzip2 block\n",
    "            if (dec_chunk_length == 0):\n",
    "                continue\n",
    "            \n",
    "            # Try to decode byte array\n",
    "            decoding_buffer += decompressed_chunk\n",
    "            try:\n",
    "                chunk_string = decoding_buffer.decode('utf-8')\n",
    "                \n",
    "                # Clear buffer\n",
    "                decoding_buffer = bytearray([])\n",
    "                \n",
    "                decoding_successful = True\n",
    "            except UnicodeDecodeError:\n",
    "                # Error occurs when input stream is split in the middle of a character which is encoded with multiple bytes\n",
    "                decoding_error_counter += 1\n",
    "                decoding_successful = False\n",
    "            \n",
    "            # Try to parse the decoded string\n",
    "            if decoding_successful:\n",
    "                # Elements of the JSON array are split by '\\n'\n",
    "                array_elements = chunk_string.split('\\n')\n",
    "                \n",
    "                # Iterate through the JSON array in the current chunk\n",
    "                for json_candidate in array_elements:\n",
    "                    # Try to parse the JSON object, might fail if the object was divided in parts because of the chunk separation\n",
    "                    parsing_buffer += json_candidate\n",
    "                    try:\n",
    "                        json_obj = json.loads(parsing_buffer)\n",
    "                        \n",
    "                        # Clear buffer\n",
    "                        parsing_buffer = ''\n",
    "                        \n",
    "                        parsing_successful = True\n",
    "                    except ValueError:\n",
    "                        \"\"\"\n",
    "                        Error occurs when the line does not contain the whole JSON object, which happens for the last array element in almost every chunk of input stream.\n",
    "                        We solve this by remembering the prevous partial objects in parsing_buffer, and then merging it with the rest of the object when we load the next chunk.\n",
    "                        \"\"\"\n",
    "                        parsing_error_counter += 1\n",
    "                        parsing_successful = False\n",
    "                    \n",
    "                    # Perform JSON object processing\n",
    "                    if parsing_successful:\n",
    "                        process_json_object(json_obj, out_json)\n",
    "            \n",
    "            # Show progress\n",
    "            total_in += len(chunk)\n",
    "            total_out += dec_chunk_length\n",
    "            if dec_chunk_length != 0:    # only if a bzip2 block emitted\n",
    "                processed_fraction = round(1000 * total_in / input_size)\n",
    "                if processed_fraction != previous_value:\n",
    "                    left = (input_size / total_in - 1) * (time.time() - start_time)\n",
    "                    print(f'\\r{processed_fraction / 10:.1f}% (~{left:.1f}s left)\\tyear: {year}\\tnumber of entries: {len(out_json)}\\tdecoding errors: {decoding_error_counter}\\tparsing errors: {parsing_error_counter}', end='      ')\n",
    "                    previous_value = processed_fraction\n",
    "    \n",
    "    # Save result to file\n",
    "    output_full_name = write_json_to_file(f'{output_name}-{year}', out_json)\n",
    "    \n",
    "    # Report ending\n",
    "    print()\n",
    "    total_time = time.time() - start_time\n",
    "    print(f'File {input_file_name} processed in {total_time:.1f}s', end='\\n\\n')\n",
    "    \n",
    "    return output_full_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "aa9598d8-a4de-4103-85b2-ec86c44218a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_if_party_member_quote(row: dict, party_member_quotes: dict, party_list: list) -> None:\n",
    "    \"\"\"CHeck if party member quote is useful for analysis\n",
    "\n",
    "    Args:\n",
    "        row (dict): Row of data\n",
    "        party_member_quotes (dict): Dict to keep track of party member quotes\n",
    "        party_list (list): Party list\n",
    "    \"\"\"\n",
    "    probabilities = row['probas']\n",
    "    qids = row['qids']\n",
    "    \n",
    "    # Check if the probas and qids values exist\n",
    "    if (len(probabilities) == 0 or len(qids) == 0):\n",
    "        return\n",
    "    \n",
    "    # Check if the speaker is not 'Unknown'\n",
    "    if (probabilities[0][0] == 'None'):\n",
    "        return\n",
    "    \n",
    "    # Check if the probability is over 80%\n",
    "    p = float(probabilities[0][1])\n",
    "    if (p < 0.8):\n",
    "        return\n",
    "    \n",
    "    # Check if the speaker is on the party list\n",
    "    qid = qids[0]\n",
    "    if qid not in party_list:\n",
    "        return\n",
    "    \n",
    "    # Remember only the quote and the probability\n",
    "    data = {}\n",
    "    data['quotation'] = row['quotation']\n",
    "    data['proba'] = row['probas'][0][1]\n",
    "    \n",
    "    # Append the quote\n",
    "    arr = party_member_quotes.get(qid, [])\n",
    "    arr.append(data)\n",
    "    party_member_quotes[qid] = arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ad38b563-995f-43b8-a544-392f554059bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define partial function check_if_dem_or_rep_quote using function check_if_party_member_quote\n",
    "check_if_top_1000_speaker_quote = partial(check_if_party_member_quote, party_list=top1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "771b86d4-cd8f-4e70-b15b-80450eb24db9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.0% (~0.4s left)\tyear: 2015\tnumber of entries: 995\tdecoding errors: 1\tparsing errors: 3187       \n",
      "File ../quotebank/quotes-2015.json.bz2 processed in 1861.2s\n",
      "\n",
      "100.0% (~0.5s left)\tyear: 2016\tnumber of entries: 998\tdecoding errors: 0\tparsing errors: 2216       \n",
      "File ../quotebank/quotes-2016.json.bz2 processed in 1557.5s\n",
      "\n",
      "100.0% (~1.3s left)\tyear: 2017\tnumber of entries: 1000\tdecoding errors: 0\tparsing errors: 4959        \n",
      "File ../quotebank/quotes-2017.json.bz2 processed in 3299.7s\n",
      "\n",
      "100.0% (~0.7s left)\tyear: 2018\tnumber of entries: 999\tdecoding errors: 1\tparsing errors: 4585        \n",
      "File ../quotebank/quotes-2018.json.bz2 processed in 1483.0s\n",
      "\n",
      "100.0% (~0.3s left)\tyear: 2019\tnumber of entries: 999\tdecoding errors: 0\tparsing errors: 3396       \n",
      "File ../quotebank/quotes-2019.json.bz2 processed in 1090.9s\n",
      "\n",
      "100.0% (~0.1s left)\tyear: 2020\tnumber of entries: 993\tdecoding errors: 0\tparsing errors: 792       \n",
      "File ../quotebank/quotes-2020.json.bz2 processed in 263.5s\n",
      "\n",
      "\n",
      "\n",
      "Output file names:\n",
      "data/local_only/top-1000-quotes-2015_1638929459545.json\n",
      "data/local_only/top-1000-quotes-2016_1638931024701.json\n",
      "data/local_only/top-1000-quotes-2017_1638934319108.json\n",
      "data/local_only/top-1000-quotes-2018_1638935809502.json\n",
      "data/local_only/top-1000-quotes-2019_1638936901748.json\n",
      "data/local_only/top-1000-quotes-2020_1638937168661.json\n"
     ]
    }
   ],
   "source": [
    "years = [2015, 2016, 2017, 2018, 2019, 2020]\n",
    "# years = [2018, 2019, 2020]\n",
    "# years = [2020]\n",
    "\n",
    "output_list = []\n",
    "\n",
    "for year in years:\n",
    "    path_to_input = PATTERN_INPUT.format(year)\n",
    "    \n",
    "    # Process quote file\n",
    "    output_name = process_compressed_json_file(path_to_input, 'data/local_only/top-1000-quotes', year, check_if_top_1000_speaker_quote)\n",
    "    \n",
    "    output_list.append(output_name)\n",
    "\n",
    "print('\\n\\nOutput file names:')\n",
    "for file_name in output_list:\n",
    "    print(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4688a10b-73e6-4d59-bb97-8251efe8f057",
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECT DISTINCT ?item ?itemLabel ?genderLabel ?citizenshipLabel ?languageLabel ?religionLabel ?ethnicLabel ?degreeLabel ?dateOfBirth ?placeOfBirthLabel\n",
    "WHERE {\n",
    "  SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\". }\n",
    "  {\n",
    "    VALUES ?item { wd:Q22686 wd:Q359442 wd:Q426582 }\n",
    "    \n",
    "    OPTIONAL { ?item wdt:P21 ?gender. }\n",
    "    OPTIONAL { ?item wdt:P27 ?citizenship. }\n",
    "    OPTIONAL { ?item wdt:P569 ?dateOfBirth. }\n",
    "    OPTIONAL { ?item wdt:P103 ?language. }\n",
    "    OPTIONAL { ?item wdt:P140 ?religion. }\n",
    "    OPTIONAL { ?item wdt:P172 ?ethnic. }\n",
    "    OPTIONAL { ?item wdt:P512 ?degree. }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cdb81b8f-1250-454d-b369-55d245b8ffe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/local_only/top-1000-wikidata-raw.json\", \"r\", encoding='utf-8') as f:\n",
    "    top1000_wiki = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f9949886-af37-4d61-ae39-fee6af201b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "top1000_wiki_merged = dict()\n",
    "\n",
    "index = 0\n",
    "for row in top1000_wiki:\n",
    "    # Extract the QID from the link (ex. http://www.wikidata.org/entity/Q203286 -> Q203286)\n",
    "    qid_start = row['item'].rindex('/') + 1\n",
    "    key = row['item'][qid_start:]\n",
    "    # Replace the link with the QID\n",
    "    row['item'] = key\n",
    "    \n",
    "    if key in top1000_wiki_merged:\n",
    "        merged_entry = top1000_wiki_merged[key]\n",
    "        columns = ['itemLabel', 'genderLabel', 'citizenshipLabel', 'religionLabel', 'ethnicLabel', 'degreeLabel', 'dateOfBirth', 'languageLabel']\n",
    "        \"\"\"\n",
    "        Merge the values for every column:\n",
    "            - if the values are the same - do nothing\n",
    "            - if the values are different - create a list and add them both\n",
    "        \"\"\"\n",
    "        for col in columns:\n",
    "            if row.get(col, None) is None:\n",
    "                continue\n",
    "                \n",
    "            updated_entry = merged_entry.get(col, None)\n",
    "            \n",
    "            if updated_entry is None:\n",
    "                updated_entry = row[col]\n",
    "            elif isinstance(updated_entry, list):\n",
    "                if row[col] not in updated_entry:\n",
    "                    updated_entry.append(row[col])\n",
    "            elif row[col] != updated_entry:\n",
    "                updated_entry = [updated_entry, row[col]]\n",
    "                \n",
    "            merged_entry[col] = updated_entry\n",
    "    else:\n",
    "        top1000_wiki_merged[key] = row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "575f489c-57b3-45a3-8a72-2556d82c61b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1223\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "print(len(top1000_wiki))\n",
    "print(len(top1000_wiki_merged))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "be22d26d-3dc1-4bf7-a041-fa57350fc423",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/local_only/top-1000-wikidata-merged_1638962423221.json'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "write_json_to_file('data/local_only/top-1000-wikidata-merged', top1000_wiki_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a4af6d66-e870-4a5f-b0d3-70f87e3d159d",
   "metadata": {},
   "outputs": [],
   "source": [
    "top1000_list = list(top1000_wiki_merged.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e0cd7326-6502-4bfe-8789-3f82ab29d4f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/local_only/top-1000-quotes-combined_1638962737072.json'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quotes_files = [\n",
    "    \"data/local_only/top-1000-quotes-2015_1638929459545.json\",\n",
    "    \"data/local_only/top-1000-quotes-2016_1638931024701.json\",\n",
    "    \"data/local_only/top-1000-quotes-2017_1638934319108.json\",\n",
    "    \"data/local_only/top-1000-quotes-2018_1638935809502.json\",\n",
    "    \"data/local_only/top-1000-quotes-2019_1638936901748.json\",\n",
    "    \"data/local_only/top-1000-quotes-2020_1638937168661.json\"\n",
    "]\n",
    "\n",
    "quotes_combined = {}\n",
    "\n",
    "for v in top1000_list:\n",
    "    copy = dict(v)\n",
    "    copy['quotations'] = []\n",
    "    \n",
    "    quotes_combined[v['item']] = copy\n",
    "\n",
    "for file_name in quotes_files:\n",
    "    with open(file_name, 'r', encoding='utf-8') as f:\n",
    "        quotes = json.load(f)\n",
    "        \n",
    "        for k in quotes.keys():\n",
    "            quotes_combined[k]['quotations'] += quotes[k]\n",
    "\n",
    "write_json_to_file('data/local_only/top-1000-quotes-combined', quotes_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f61af180-5a0f-4713-b9f9-ee99de57c674",
   "metadata": {},
   "outputs": [],
   "source": [
    "quotes_filtered = quotes_combined.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c7ff8e04-a703-40db-a15d-dd6eff53b8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_quotes = []\n",
    "\n",
    "weird_pattern = '[_@#+&;:\\(\\)\\{\\}\\[\\]\\\\/`]'\n",
    "json_pattern = '\\{.*[a-zA-Z]+:\\s[\\'\"`][a-zA-Z0-9]+[\\'\"`].*\\}'\n",
    "url_pattern = 'https?'\n",
    "\n",
    "for k in quotes_filtered.keys():\n",
    "    elem = quotes_filtered[k]\n",
    "    \n",
    "    new_arr = []\n",
    "    for entry in elem['quotations']:\n",
    "        text = entry['quotation']\n",
    "        \n",
    "        longest = max(entry['quotation'].split(), key=len)\n",
    "        if (len(longest) > 50):\n",
    "            filtered_quotes.append(entry)\n",
    "            continue\n",
    "        \n",
    "        if re.search(url_pattern, text) is not None:\n",
    "            filtered_quotes.append(entry)\n",
    "            continue\n",
    "        \n",
    "        if re.search(json_pattern, text) is not None:\n",
    "            filtered_quotes.append(entry)\n",
    "            continue\n",
    "            \n",
    "        weird_num = len(re.findall(weird_pattern, text))\n",
    "        total = len(text)\n",
    "        weird_percent = weird_num / total\n",
    "        if (weird_percent > 0.1):\n",
    "            filtered_quotes.append(entry)\n",
    "            continue\n",
    "            \n",
    "        new_arr.append(entry)\n",
    "    elem['quotations'] = new_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9198b697-6d65-4db1-9e0b-0f96b8e5f1c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/local_only/top-1000-quotes-combined-and-filtered_1638963965624.json'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "write_json_to_file('data/local_only/top-1000-quotes-combined-and-filtered', quotes_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e847e0-9c7c-413f-86b7-5c5c578e7aec",
   "metadata": {},
   "source": [
    "Show some filtered quotes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "078e191d-2f91-4e3a-83fa-3abae9593c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "absolutely love (s)\n",
      "\n",
      "Finally @aamir_khan ki Masti Ki Paathshala is open & #KaategiKya is our super hit school anthem! पसंद आया तो लाइक करो पसंद नहीं आया तो टेस्ट चेंज करो!?? #AamirsMastiKiPaathshala #TeachersDay #शिक्षकदिवस Wah! Kaun hai bhai iss group ka principal? Whoever made this video, lots of love to you.. https://t.co/9fq8pSpH3z -- Aamir Khan (@aamir_khan) September 5, 2018 Aamir quote tweeted the video and said,\n",
      "\n",
      "Sen. John Mc & shy; Cain took on two key 2016 Re & shy; pub & shy; lic & shy; an con & shy; tenders Wed & shy; nes & shy; day, ex & shy; press & shy; ing baffle & shy; ment at Don & shy; ald Trump's con & shy; tin & shy; ued pop & shy; ular & shy; ity and cri & shy; ti & shy; ciz & shy; ing Ted Cruz for his tac & shy; tics in the Sen & shy; ate. Mc & shy; Cain, the 2008 GOP pres & shy; id & shy; en & shy; tial nom & shy; in & shy; ee, gave his view of the 2016 field be & shy; fore a break & shy; fast meet & shy; ing with re & shy; port & shy; ers on Wed & shy; nes & shy; day, caveat & shy; ing with typ & shy; ic & shy; al hu & shy; mor that his pre & shy; dic & shy; tions, `speak & shy; ing as the loser,' de & shy; serve `zero cred & shy; ib & shy; il & shy; ity. '\n",
      "\n",
      "like [ s ] a lot of\n",
      "\n",
      "[ didn't ] think\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for entry in filtered_quotes[0:5]:\n",
    "    print(entry['quotation'], end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "585bf8ca-0d2b-4827-b7de-ace8b3b0f3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/local_only/top-1000-quotes-combined-and-filtered_1638963965624.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e7b89b33-76b2-46de-ae43-24f8ff6aea3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q9557            2871\n",
      "Q207431          8317\n",
      "Q244338          3242\n",
      "Q483309          2737\n",
      "Q1382365         4098\n",
      "Q2834185         2840\n",
      "Q3595385         5687\n",
      "Q6968942         2770\n",
      "Q16196017        5309\n",
      "Q11310708        5687\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-7346b80c0199>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m11\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkeys\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'{data[key][\"item\"]:10} {len(data[key][\"quotations\"]):10}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "keys = list(data.keys())\n",
    "\n",
    "for i in range(11):\n",
    "    key = keys[i * 100]\n",
    "    print(f'{data[key][\"item\"]:10} {len(data[key][\"quotations\"]):10}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3ab9f6bb-3876-4963-91d2-b46489cbe1ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "quotes_concat = quotes_filtered.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "838ca53f-81d6-478d-a6dc-de0aeae3f799",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUOTE_LENGTH = 5000\n",
    "\n",
    "index = 1\n",
    "for k in quotes_concat.keys():\n",
    "    elem = quotes_concat[k]\n",
    "    \n",
    "    # Sort the quotes by length\n",
    "    elem['quotations'].sort(key = lambda x: len(x['quotation']), reverse = True)\n",
    "    \n",
    "    concat = ''\n",
    "    printed = False\n",
    "    for quote in elem['quotations']:\n",
    "        # Concatenate the quotes\n",
    "        concat += ' ' + quote['quotation']\n",
    "        \n",
    "        # Trim if we are over QUOTE_LENGTH\n",
    "        if (len(concat) >= QUOTE_LENGTH):\n",
    "            concat = concat[0:QUOTE_LENGTH]\n",
    "            break\n",
    "    \n",
    "    elem['quotations'] = concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a1e739ba-0b80-42b0-b8d3-b2b657f3d5f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/local_only/top-1000-quotes-concatenated_1638964313725.json'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "write_json_to_file('data/local_only/top-1000-quotes-concatenated', quotes_concat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed327727-f076-4380-90a6-14cbee61ed1c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1115d421-f01d-4722-a2b2-45530c991a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bz2\n",
    "import json\n",
    "import pandas as pd\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "017dfa1f-6ede-4745-91be-18d8a4c3e45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the concatenated quotes for top 100 politician\n",
    "with open('data/local_only/top-1000-quotes-concatenated_1638964313725.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123ea2cf-c88f-4eea-8d1e-2701a1cf32d5",
   "metadata": {},
   "source": [
    "After getting the data, we extract the quote ID and the concatenated quote of each politician, and write them to `input_data1.csv` for the LIWC personality analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2e1150d8-f7ac-418f-8ed5-957690799fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/local_only/top_1000_input_data_1.csv', 'w', encoding='UTF8', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"qid\", \"quote\"])\n",
    "    for qid, all_value in data.items():\n",
    "        quote = all_value[\"quotations\"]\n",
    "        writer.writerow([qid, quote])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "287400d0-4a20-4963-9edb-880758ff98aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "liwc = pd.read_csv('data/local_only/top_1000_output_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5cb40fbd-e83c-43bd-8f06-8d1abb03c0fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source (A)</th>\n",
       "      <th>Source (B)</th>\n",
       "      <th>WC</th>\n",
       "      <th>WPS</th>\n",
       "      <th>Sixltr</th>\n",
       "      <th>Dic</th>\n",
       "      <th>Pronoun</th>\n",
       "      <th>I</th>\n",
       "      <th>We</th>\n",
       "      <th>Self</th>\n",
       "      <th>...</th>\n",
       "      <th>Comma</th>\n",
       "      <th>Colon</th>\n",
       "      <th>SemiC</th>\n",
       "      <th>QMark</th>\n",
       "      <th>Exclam</th>\n",
       "      <th>Dash</th>\n",
       "      <th>Quote</th>\n",
       "      <th>Apostro</th>\n",
       "      <th>Parenth</th>\n",
       "      <th>OtherP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>Q717959</td>\n",
       "      <td>At the moment we're all feeling disappointed ...</td>\n",
       "      <td>927</td>\n",
       "      <td>23.77</td>\n",
       "      <td>15.64</td>\n",
       "      <td>76.27</td>\n",
       "      <td>10.57</td>\n",
       "      <td>1.4</td>\n",
       "      <td>3.56</td>\n",
       "      <td>4.96</td>\n",
       "      <td>...</td>\n",
       "      <td>3.88</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.34</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 86 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Source (A)                                         Source (B)   WC    WPS  \\\n",
       "255    Q717959   At the moment we're all feeling disappointed ...  927  23.77   \n",
       "\n",
       "     Sixltr    Dic  Pronoun    I    We  Self  ...  Comma  Colon  SemiC  QMark  \\\n",
       "255   15.64  76.27    10.57  1.4  3.56  4.96  ...   3.88   0.11   0.11    0.0   \n",
       "\n",
       "     Exclam  Dash  Quote  Apostro  Parenth  OtherP  \n",
       "255     0.0   1.4    0.0     3.34     0.65     0.0  \n",
       "\n",
       "[1 rows x 86 columns]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualise a random sample\n",
    "liwc.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc208021-444e-444f-be48-4c6cc6b48a64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
