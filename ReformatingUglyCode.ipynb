{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76943fef-2ba9-4e78-a18c-022695df3e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bz2\n",
    "import json\n",
    "import re\n",
    "import random\n",
    "import sys\n",
    "import os\n",
    "import bz2\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b693e4f7-43e0-4058-8124-b61e01ee1dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATTERN_INPUT = \"../quotebank/quotes-{}.json.bz2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61bd80a3-1c75-44ca-8271-398bfeae1e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_json_to_file(name, obj):\n",
    "    # Use current timestamp to make the name of the file unique\n",
    "    millis = round(time.time() * 1000)\n",
    "    name = f'{name}_{millis}.json'\n",
    "    with open(name, 'wb') as f:\n",
    "        output = json.dumps(obj)\n",
    "        f.write(output.encode('utf-8'))\n",
    "    return name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa21b81-5eea-47db-90c1-22fecd8eb8fe",
   "metadata": {},
   "source": [
    "# General"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d17a7f7-c093-4568-a299-368789343944",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Process a chunk of the input stream.\n",
    "\"\"\"\n",
    "def proc(input, evaluate_quote, max_length=20):\n",
    "    # Ugly global variable usage :(\n",
    "    global index\n",
    "    global invalid_json_count\n",
    "    global invalid_chunk_count\n",
    "    global chunk_stitching\n",
    "    global stitch_length\n",
    "    global scrap_next\n",
    "    global quote_is_open\n",
    "    global quote_part\n",
    "    global dat_part\n",
    "    global euro_error\n",
    "    global euro_count\n",
    "    \n",
    "    global totin\n",
    "    global totout\n",
    "    global prev\n",
    "    global dec\n",
    "    global start\n",
    "    \"\"\"Decompress and process a piece of a compressed stream\"\"\"\n",
    "    dat = dec.decompress(input)\n",
    "    got = len(dat)\n",
    "    if got != 0:    # 0 is common -- waiting for a bzip2 block\n",
    "        try:\n",
    "            if (euro_error):\n",
    "                # If the previous chunk ended unexpectedly and could not be decoded, try to combine it with this chunk\n",
    "                s = (dat_part + dat).decode('utf-8')\n",
    "                euro_error = False\n",
    "            else:\n",
    "                # Decode the current chunk\n",
    "                s = dat.decode('utf-8')\n",
    "                \n",
    "            # List elements in the quote files are separated by new lines (\\n)\n",
    "            lines = s.split('\\n')\n",
    "\n",
    "            for line in lines:\n",
    "                try:\n",
    "                    if (scrap_next):\n",
    "                        # If the object spans too many chunks we decide to scrap it, and keep scraping until JSON can parse the line (chunk)\n",
    "                        ob = json.loads(line)\n",
    "                        scrap_next = False\n",
    "                        quote_is_open = False\n",
    "                        chunk_stitching -= stitch_length\n",
    "                    else:\n",
    "                        if (quote_is_open):\n",
    "                            # If previous chunk ended in the middle of a JSON object we merge that content with the current line\n",
    "                            ob = json.loads(quote_part + line)\n",
    "                            quote_is_open = False\n",
    "                        else:\n",
    "                            # Parse the current line\n",
    "                            ob = json.loads(line)\n",
    "\n",
    "                    # Parametrization - do work on a single quote JSON object\n",
    "                    evaluate_quote({}, ob)\n",
    "                except ValueError:\n",
    "                    \"\"\"\n",
    "                    Error occurs when the line does not contain the whole JSON object, which happens for the last line in almost every chunk of input stream.\n",
    "                    We solve this by remembering the partial object, and then merging it with the rest of the object when we load the next chunk.\n",
    "                    JSON object might span more than 2 chunks, and in that case we keep merging until we reach max_length chunks, when we just throw away the object\n",
    "                    and count it as invalid using invalid_json_count.\n",
    "                    \"\"\"\n",
    "                    if (scrap_next):\n",
    "                        pass\n",
    "                    else:\n",
    "                        if (quote_is_open):\n",
    "                            chunk_stitching += 1\n",
    "                            quote_part = quote_part + line\n",
    "                            stitch_length += 1\n",
    "\n",
    "                            if (stitch_length > max_length):\n",
    "                                invalid_json_count += 1\n",
    "                                scrap_next = True\n",
    "                        else:\n",
    "                            quote_is_open = True\n",
    "                            quote_part = line\n",
    "                            stitch_length = 0\n",
    "        except UnicodeDecodeError as e:\n",
    "            # Error occurs when input stream is split in the middle of a character which is encoded with multiple bytes, for example the euro symbol\n",
    "            if (euro_error):\n",
    "                dat_part = dat_part + dat\n",
    "            else:\n",
    "                euro_error = True\n",
    "                dat_part = dat\n",
    "            \n",
    "            euro_count += 1\n",
    "        \n",
    "        index += 1\n",
    "    return got"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57321fb5-cab3-4e68-8fd5-64ffafaa7a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_through_quotes(init, evaluate_quote, year, target_dict_name, path_to_input, name='test', chunk_size=16384):\n",
    "    global index\n",
    "    global invalid_json_count\n",
    "    global invalid_chunk_count\n",
    "    global chunk_stitching\n",
    "    global stitch_length\n",
    "    global scrap_next\n",
    "    global quote_is_open\n",
    "    global quote_part\n",
    "    global dat_part\n",
    "    global euro_error\n",
    "    global euro_count\n",
    "    \n",
    "    global totin\n",
    "    global totout\n",
    "    global prev\n",
    "    global dec\n",
    "    global start\n",
    "    \n",
    "    size = os.path.getsize(path_to_input)\n",
    "    invalid_json_count = 0\n",
    "    invalid_chunk_count = 0\n",
    "    chunk_stitching = 0\n",
    "    stitch_length = 0\n",
    "    scrap_next = False\n",
    "    quote_is_open = False\n",
    "    quote_part = ''\n",
    "    dat_part = 0\n",
    "    euro_error = False\n",
    "    euro_count = 0\n",
    "    \n",
    "    totin = 0\n",
    "    totout = 0\n",
    "    prev = -1\n",
    "    dec = bz2.BZ2Decompressor()\n",
    "    start = time.time()\n",
    "    \n",
    "    init({})\n",
    "    \n",
    "    target_dict = poli_quotes if target_dict_name == \"poli_quotes\" else signi_quote_dict\n",
    "    index = 0\n",
    "    with open(path_to_input, 'rb') as f:\n",
    "        for chunk in iter(lambda: f.read(chunk_size), b''):\n",
    "            # feed chunk to decompressor\n",
    "            got = proc(chunk, evaluate_quote)\n",
    "\n",
    "            # handle case of concatenated bz2 streams\n",
    "            if dec.eof:\n",
    "                rem = dec.unused_data\n",
    "                dec = bz2.BZ2Decompressor()\n",
    "                got += proc(rem, evaluate_quote)\n",
    "\n",
    "            # show progress\n",
    "            totin += len(chunk)\n",
    "            totout += got\n",
    "            if got != 0:    # only if a bzip2 block emitted\n",
    "                frac = round(1000 * totin / size)\n",
    "                if frac != prev:\n",
    "                    left = (size / totin - 1) * (time.time() - start)\n",
    "                    print(f'\\r{frac / 10:.1f}% (~{left:.1f}s left)\\tyear: {year}\\tnumber of speakers: {len(target_dict)}\\tstitching: {chunk_stitching}\\teuro count: {euro_count}\\tinvalid json count: {invalid_json_count}\\tinvalid chunk count: {invalid_chunk_count}', end='')\n",
    "                    prev = frac\n",
    "\n",
    "    # Show the resulting size.\n",
    "    print(end='\\r')\n",
    "    print(totout, 'uncompressed bytes')\n",
    "\n",
    "    output_name = write_json_to_file(f'{name}-{year}', target_dict)\n",
    "    return output_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c523b36d-8102-4b71-9477-1c740b1ab756",
   "metadata": {},
   "source": [
    "### General fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "88480c61-789f-4ef0-b6e9-019eac7bb2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNK_SIZE = 1_048_576\n",
    "\n",
    "def process_compressed_json_file(input_file_name, output_name, year, process_json_object):\n",
    "    # Decompression variables\n",
    "    decompressor = bz2.BZ2Decompressor()\n",
    "    \n",
    "    # Decoding variables\n",
    "    decoding_buffer = bytearray([])\n",
    "    decoding_error_counter = 0\n",
    "    \n",
    "    # Parsing variables\n",
    "    parsing_buffer = ''\n",
    "    parsing_error_counter = 0\n",
    "    \n",
    "    # Progress variables - used to provide feedback to the dev\n",
    "    input_size = os.path.getsize(input_file_name)\n",
    "    start_time = time.time()\n",
    "    total_in = 0\n",
    "    total_out = 0\n",
    "    previous_value = -1\n",
    "    \n",
    "    # Result of processing\n",
    "    out_json = dict()\n",
    "    \n",
    "    # Iterate through the file\n",
    "    with open(input_file_name, 'rb') as input_file:\n",
    "        for chunk in iter(lambda: input_file.read(CHUNK_SIZE), b''):\n",
    "            # Feed chunk to decompressor\n",
    "            decompressed_chunk = decompressor.decompress(chunk)\n",
    "            dec_chunk_length = len(decompressed_chunk)\n",
    "            \n",
    "            # Check the length of the decompressed data - 0 is common -- waiting for a bzip2 block\n",
    "            if (dec_chunk_length == 0):\n",
    "                continue\n",
    "            \n",
    "            # Try to decode byte array\n",
    "            decoding_buffer += decompressed_chunk\n",
    "            try:\n",
    "                chunk_string = decoding_buffer.decode('utf-8')\n",
    "                \n",
    "                # Clear buffer\n",
    "                decoding_buffer = bytearray([])\n",
    "                \n",
    "                decoding_successful = True\n",
    "            except UnicodeDecodeError:\n",
    "                # Error occurs when input stream is split in the middle of a character which is encoded with multiple bytes\n",
    "                decoding_error_counter += 1\n",
    "                decoding_successful = False\n",
    "            \n",
    "            # Try to parse the decoded string\n",
    "            if decoding_successful:\n",
    "                # Elements of the JSON array are split by '\\n'\n",
    "                array_elements = chunk_string.split('\\n')\n",
    "                \n",
    "                # Iterate through the JSON array in the current chunk\n",
    "                for json_candidate in array_elements:\n",
    "                    # Try to parse the JSON object, might fail if the object was divided in parts because of the chunk separation\n",
    "                    parsing_buffer += json_candidate\n",
    "                    try:\n",
    "                        json_obj = json.loads(parsing_buffer)\n",
    "                        \n",
    "                        # Clear buffer\n",
    "                        parsing_buffer = ''\n",
    "                        \n",
    "                        parsing_successful = True\n",
    "                    except ValueError:\n",
    "                        \"\"\"\n",
    "                        Error occurs when the line does not contain the whole JSON object, which happens for the last array element in almost every chunk of input stream.\n",
    "                        We solve this by remembering the prevous partial objects in parsing_buffer, and then merging it with the rest of the object when we load the next chunk.\n",
    "                        \"\"\"\n",
    "                        parsing_error_counter += 1\n",
    "                        parsing_successful = False\n",
    "                    \n",
    "                    # Perform JSON object processing\n",
    "                    if parsing_successful:\n",
    "                        process_json_object(json_obj, out_json)\n",
    "            \n",
    "            # Show progress\n",
    "            total_in += len(chunk)\n",
    "            total_out += dec_chunk_length\n",
    "            if dec_chunk_length != 0:    # only if a bzip2 block emitted\n",
    "                processed_fraction = round(1000 * total_in / input_size)\n",
    "                if processed_fraction != previous_value:\n",
    "                    left = (input_size / total_in - 1) * (time.time() - start_time)\n",
    "                    print(f'\\r{processed_fraction / 10:.1f}% (~{left:.1f}s left)\\tyear: {year}\\tnumber of entries: {len(out_json)}\\tdecoding errors: {decoding_error_counter}\\tparsing errors: {parsing_error_counter}', end='      ')\n",
    "                    previous_value = processed_fraction\n",
    "    \n",
    "    # Save result to file\n",
    "    output_full_name = write_json_to_file(f'{output_name}-{year}', out_json)\n",
    "    \n",
    "    # Report ending\n",
    "    print()\n",
    "    total_time = time.time() - start_time\n",
    "    print(f'File {input_file_name} processed in {total_time:.1f}s', end='\\n\\n')\n",
    "    \n",
    "    return output_full_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfae515d-115d-466f-83ab-9ef32f36f898",
   "metadata": {},
   "source": [
    "# Counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61352fc3-8ca2-4f8e-a468-10a37e5b9359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The signature remains from an older version of the code, parameter out_file could be removed, but then has to be removed in other places in the code as well.\n",
    "def initialize(out_file):\n",
    "    global signi_count\n",
    "    global signi_quote_dict\n",
    "    signi_count = 0\n",
    "    signi_quote_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bbfaa408-4831-42e6-b10d-cca16bab69d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "signi_count = 0\n",
    "signi_quote_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ee5af43-9203-448a-b924-071ddf2db4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The signature remains from an older version of the code, parameter out_file could be removed, but then has to be removed in other places in the code as well.\n",
    "def count_significant_quotes(out_file, row):\n",
    "    global signi_count\n",
    "    global signi_quote_dict\n",
    "    \n",
    "    probas = row['probas']\n",
    "    qids = row['qids']\n",
    "    \n",
    "    if (len(probas) == 0 or len(qids) == 0):\n",
    "        return\n",
    "    \n",
    "    if (probas[0][0] == 'None'):\n",
    "        return\n",
    "    \n",
    "    p = float(probas[0][1])\n",
    "    if (p < 0.8):\n",
    "        return\n",
    "    \n",
    "    qid = qids[0]\n",
    "    \n",
    "    signi_count = signi_count + 1\n",
    "    signi_quote_dict[qid] = signi_quote_dict.get(qid, 0) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c16f3b3-f8bf-48c9-95cb-89cf36539b78",
   "metadata": {},
   "source": [
    "### Counting fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "332bf1ba-a6bf-4385-bc9d-6dafda27c71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOT_SIGNIFICANT_QUOTE = False, 0, 0\n",
    "\n",
    "def check_if_significant_quote(row, significant_quote_counters):\n",
    "    probabilities = row['probas']\n",
    "    qids = row['qids']\n",
    "    \n",
    "    # Check if the probas and qids values exist\n",
    "    if (len(probabilities) == 0 or len(qids) == 0):\n",
    "        return\n",
    "    \n",
    "    # Check if the speaker is not 'Unknown'\n",
    "    if (probabilities[0][0] == 'None'):\n",
    "        return\n",
    "    \n",
    "    # Check if the probability is over 80%\n",
    "    prob = float(probabilities[0][1])\n",
    "    if (prob < 0.8):\n",
    "        return\n",
    "    \n",
    "    # Increment count\n",
    "    qid = qids[0]\n",
    "    significant_quote_counters[qid] = significant_quote_counters.get(qid, 0) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b691d969-0152-45b1-80e5-e4eab175f378",
   "metadata": {},
   "source": [
    "# Quote selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd553b1f-b48a-45fb-bb37-3d5ac5706410",
   "metadata": {},
   "outputs": [],
   "source": [
    "poli_quotes = {}\n",
    "poli_people = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9bb79f1c-4e93-4cf7-ba91-43622e5866ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def poli_initialize(out_file):\n",
    "    global poli_quotes\n",
    "    global poli_people\n",
    "    global dem_list\n",
    "    global rep_list\n",
    "    \n",
    "    poli_quotes = {}\n",
    "    poli_people = set()\n",
    "    \n",
    "    for v in dem_list:\n",
    "        poli_people.add(v['item'])\n",
    "    for v in rep_list:\n",
    "        poli_people.add(v['item'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21ef05ad-cc54-404a-9acd-fd9a3cd6970a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Remember the quote, only if it belongs to one of the politicians in the set poli_people, and if the probability is over 80%.\n",
    "\"\"\"\n",
    "def save_politician_quotes(out_file, row):\n",
    "    global poli_quotes\n",
    "    global poli_people\n",
    "    \n",
    "    probas = row['probas']\n",
    "    qids = row['qids']\n",
    "    \n",
    "    # Check if the probability field exists\n",
    "    if (len(probas) == 0 or len(qids) == 0):\n",
    "        return\n",
    "    \n",
    "    if (probas[0][0] == 'None'):\n",
    "        return\n",
    "    \n",
    "    # Check if the probability is over 80%\n",
    "    p = float(probas[0][1])\n",
    "    if (p < 0.8):\n",
    "        return\n",
    "    \n",
    "    # Check if the speaker is one of the 100 party members\n",
    "    qid = qids[0]\n",
    "    if qid not in poli_people:\n",
    "        return\n",
    "    \n",
    "    # Remember only the quote and the probability\n",
    "    data = {}\n",
    "    data['quotation'] = row['quotation']\n",
    "    data['proba'] = row['probas'][0][1]\n",
    "    \n",
    "    # Append the quote\n",
    "    arr = poli_quotes.get(qid, [])\n",
    "    arr.append(data)\n",
    "    poli_quotes[qid] = arr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce4ec3b-6412-4f80-a5f0-3178459e3dcd",
   "metadata": {},
   "source": [
    "### Quote selection fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e5d84eec-7ac1-42bf-beff-3a8cddd3403b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOT_PARTY_MEMBER_QUOTE = False, 0, 0\n",
    "\n",
    "def check_if_party_member_quote(row, party_member_quotes, party_list):\n",
    "    probabilities = row['probas']\n",
    "    qids = row['qids']\n",
    "    \n",
    "    # Check if the probas and qids values exist\n",
    "    if (len(probabilities) == 0 or len(qids) == 0):\n",
    "        return\n",
    "    \n",
    "    # Check if the speaker is not 'Unknown'\n",
    "    if (probabilities[0][0] == 'None'):\n",
    "        return\n",
    "    \n",
    "    # Check if the probability is over 80%\n",
    "    p = float(probabilities[0][1])\n",
    "    if (p < 0.8):\n",
    "        return\n",
    "    \n",
    "    # Check if the speaker is on the party list\n",
    "    qid = qids[0]\n",
    "    if qid not in party_list:\n",
    "        return\n",
    "    \n",
    "    # Remember only the quote and the probability\n",
    "    data = {}\n",
    "    data['quotation'] = row['quotation']\n",
    "    data['proba'] = row['probas'][0][1]\n",
    "    \n",
    "    # Append the quote\n",
    "    arr = party_member_quotes.get(qid, [])\n",
    "    arr.append(data)\n",
    "    party_member_quotes[qid] = arr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a2db14-a011-4ddb-9fe8-87c4dd1e4ae4",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "36c8fe79-9840-4b1c-b7ac-d857dfbf8188",
   "metadata": {},
   "outputs": [],
   "source": [
    "do_counting = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8c8c1938-2e97-42cd-bdcd-093a915b0854",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dem_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-16f95ce5b625>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[1;31m# Quote selection\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         run_through_quotes(\n\u001b[0m\u001b[0;32m     17\u001b[0m             poli_initialize, save_politician_quotes, year, \"poli_quotes\", path_to_input, name='politician-quotes', chunk_size=1_048_576)\n\u001b[0;32m     18\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-ce43127d5cec>\u001b[0m in \u001b[0;36mrun_through_quotes\u001b[1;34m(init, evaluate_quote, year, target_dict_name, path_to_input, name, chunk_size)\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m     \u001b[0minit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[0mtarget_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpoli_quotes\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mtarget_dict_name\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"poli_quotes\"\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0msigni_quote_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-447ad36063a1>\u001b[0m in \u001b[0;36mpoli_initialize\u001b[1;34m(out_file)\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mpoli_people\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdem_list\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[0mpoli_people\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'item'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrep_list\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dem_list' is not defined"
     ]
    }
   ],
   "source": [
    "# years = [2015, 2016, 2017, 2018, 2019, 2020]\n",
    "years = [2020]\n",
    "for year in years:\n",
    "    path_to_input = PATTERN_INPUT.format(year)\n",
    "    \n",
    "    if do_counting:\n",
    "        # Counting\n",
    "#         run_through_quotes(\n",
    "#             initialize, count_significant_quotes, year, \"signi_quote_dict\", path_to_input, name='signi-quote-count', chunk_size=1_048_576)\n",
    "        run_through_quotes(\n",
    "            initialize, check_if_significant_quote, year, \"signi_quote_dict\", path_to_input, name='signi-quote-count', chunk_size=1_048_576)\n",
    "        print('')\n",
    "        print(f'Finished counting quotes for the year {year}')\n",
    "    else:\n",
    "        # Quote selection\n",
    "        run_through_quotes(\n",
    "            poli_initialize, save_politician_quotes, year, \"poli_quotes\", path_to_input, name='politician-quotes', chunk_size=1_048_576)\n",
    "        print('')\n",
    "        print(f'Finished compiling quotes for the year {year}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa6f537-9194-4250-b941-fe0034b4a58c",
   "metadata": {},
   "source": [
    "### Fix testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b8f104dc-bdcf-49be-894a-d4ac9dd360ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_counting_fix = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6c05edd5-9dc2-4517-a775-b8d86a9c22f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "# Get dem_list and rep_list\n",
    "with open('../quotebank/top100_politicians_by_party.json', 'r') as f:\n",
    "    party_lists = json.load(f)\n",
    "\n",
    "dem_list = party_lists['dem']\n",
    "rep_list = party_lists['rep']\n",
    "    \n",
    "# Join both party lists\n",
    "dem_and_rep_set = set()    \n",
    "for v in dem_list:\n",
    "    dem_and_rep_set.add(v['item'])\n",
    "for v in rep_list:\n",
    "    dem_and_rep_set.add(v['item'])\n",
    "\n",
    "# Define partial function check_if_dem_or_rep_quote using function check_if_party_member_quote\n",
    "check_if_dem_or_rep_quote = partial(check_if_party_member_quote, party_list=dem_and_rep_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "85735ea4-d389-445b-ad56-37fad4b05311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting quotes\n",
      "100.0% (~0.1s left)\tyear: 2020\tnumber of entries: 171497\tdecoding errors: 0\tparsing errors: 792       \n",
      "File ../quotebank/quotes-2020.json.bz2 processed in 235.1s\n",
      "\n",
      "\n",
      "\n",
      "Output file names:\n",
      "data/signi-quote-count-2020_1636656766089.json\n"
     ]
    }
   ],
   "source": [
    "# years = [2015, 2016, 2017, 2018, 2019, 2020]\n",
    "years = [2020]\n",
    "\n",
    "output_list = []\n",
    "\n",
    "print('Counting quotes' if test_counting_fix else 'Saving politician quotes')\n",
    "for year in years:\n",
    "    path_to_input = PATTERN_INPUT.format(year)\n",
    "    \n",
    "    # Process quote files\n",
    "    if test_counting_fix:\n",
    "        short_name = 'data/signi-quote-count'\n",
    "        process_quote = check_if_significant_quote\n",
    "        \n",
    "        # Counting fix\n",
    "#         output_name = process_compressed_json_file(path_to_input, 'data/signi-quote-count', year, check_if_significant_quote)\n",
    "#         output_list.append(output_name)\n",
    "    else:\n",
    "        short_name = 'data/politician-quotes'\n",
    "        process_quote = check_if_dem_or_rep_quote\n",
    "        \n",
    "        # Quote selection fix\n",
    "#         output_name = process_compressed_json_file(path_to_input, '', year, check_if_dem_or_rep_quote)\n",
    "#         output_list.append(output_name)\n",
    "\n",
    "    output_name = process_compressed_json_file(path_to_input, short_name, year, process_quote)\n",
    "    output_list.append(output_name)\n",
    "\n",
    "print('\\n\\nOutput file names:')\n",
    "for file_name in output_list:\n",
    "    print(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1872192d-823c-4401-91f5-4b1bc4412fce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
